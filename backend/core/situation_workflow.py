"""
ìƒí™©ë¶„ì„ LangGraph ì›Œí¬í”Œë¡œìš°
ë‹¨ì¼ ìŠ¤í… â†’ ë©€í‹° ìŠ¤í… ëª¨ë“ˆí˜• ê·¸ë˜í”„ ê¸°ë°˜ ì‹¤í–‰
"""

from typing import TypedDict, List, Optional, Dict, Any
import asyncio
import logging
import json
import re

logger = logging.getLogger(__name__)

try:
    from langgraph.graph import StateGraph, END
    LANGGRAPH_AVAILABLE = True
except ImportError:
    LANGGRAPH_AVAILABLE = False
    logger.warning("LangGraphê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install langgraphë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")

from models.schemas import LegalGroundingChunk, LegalCasePreview
from core.supabase_vector_store import SupabaseVectorStore
from core.generator_v2 import LLMGenerator
from core.prompts import (
    build_situation_classify_prompt,
    build_situation_action_guide_prompt,
)

logger = logging.getLogger(__name__)


# ============================================================================
# State ëª¨ë¸ ì •ì˜
# ============================================================================

class SituationWorkflowState(TypedDict):
    """ìƒí™©ë¶„ì„ ì›Œí¬í”Œë¡œìš° ìƒíƒœ"""
    # ì…ë ¥ ë°ì´í„°
    situation_text: str
    category_hint: Optional[str]
    summary: Optional[str]
    details: Optional[str]
    employment_type: Optional[str]
    work_period: Optional[str]
    weekly_hours: Optional[int]
    is_probation: Optional[bool]
    social_insurance: Optional[str]
    
    # ì¤‘ê°„ ê²°ê³¼
    query_text: Optional[str]  # summary + details ë˜ëŠ” situation_text
    query_embedding: Optional[List[float]]  # ì„ë² ë”© ë²¡í„°
    
    # ë¶„ë¥˜ ê²°ê³¼
    classification: Optional[Dict[str, Any]]  # {classified_type, risk_score, categories}
    
    # ê·œì • í•„í„°ë§ ê²°ê³¼
    filtered_categories: Optional[List[str]]  # ê²€ìƒ‰í•  ì¹´í…Œê³ ë¦¬ ëª©ë¡
    
    # RAG ê²€ìƒ‰ ê²°ê³¼
    grounding_chunks: Optional[List[LegalGroundingChunk]]  # ë²•ë ¹/ë§¤ë‰´ì–¼
    related_cases: Optional[List[LegalCasePreview]]  # ì¼€ì´ìŠ¤
    legal_basis: Optional[List[Dict[str, Any]]]  # ë²•ì  ê·¼ê±° êµ¬ì¡° (criteria ê°€ê³µìš©)
    
    # ì•¡ì…˜ ê°€ì´ë“œ ìƒì„± ê²°ê³¼
    action_plan: Optional[Dict[str, Any]]  # {steps: [...]}
    scripts: Optional[Dict[str, str]]  # {to_company, to_advisor}
    criteria: Optional[List[Dict[str, Any]]]  # ë²•ì  íŒë‹¨ ê¸°ì¤€
    
    # ìµœì¢… ê²°ê³¼
    summary_report: Optional[str]  # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë¦¬í¬íŠ¸
    final_output: Optional[Dict[str, Any]]  # ìµœì¢… JSON ì¶œë ¥


# ============================================================================
# ì›Œí¬í”Œë¡œìš° ë…¸ë“œ ì •ì˜
# ============================================================================

class SituationWorkflow:
    """ìƒí™©ë¶„ì„ LangGraph ì›Œí¬í”Œë¡œìš°"""
    
    def __init__(self):
        if not LANGGRAPH_AVAILABLE:
            raise ImportError("LangGraphê°€ í•„ìš”í•©ë‹ˆë‹¤. pip install langgraphë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        self.vector_store = SupabaseVectorStore()
        self.generator = LLMGenerator()
        self.graph = self._build_graph()
    
    def _build_graph(self) -> StateGraph:
        """ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ êµ¬ì„±"""
        workflow = StateGraph(SituationWorkflowState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("prepare_query", self.prepare_query_node)
        workflow.add_node("classify_situation", self.classify_situation_node)
        workflow.add_node("filter_rules", self.filter_rules_node)
        workflow.add_node("retrieve_guides", self.retrieve_guides_node)
        workflow.add_node("generate_action_guide", self.generate_action_guide_node)
        workflow.add_node("merge_output", self.merge_output_node)
        
        # ì—£ì§€ ì •ì˜
        workflow.set_entry_point("prepare_query")
        workflow.add_edge("prepare_query", "classify_situation")
        workflow.add_edge("classify_situation", "filter_rules")
        workflow.add_edge("filter_rules", "retrieve_guides")
        workflow.add_edge("retrieve_guides", "generate_action_guide")
        workflow.add_edge("generate_action_guide", "merge_output")  # generate_summary ì œê±°
        workflow.add_edge("merge_output", END)
        
        return workflow.compile()
    
    # ==================== ë…¸ë“œ í•¨ìˆ˜ë“¤ ====================
    
    async def prepare_query_node(self, state: SituationWorkflowState) -> SituationWorkflowState:
        """1. ì¿¼ë¦¬ í…ìŠ¤íŠ¸ ì¤€ë¹„ ë° ì„ë² ë”© ìƒì„±"""
        logger.info("[ì›Œí¬í”Œë¡œìš°] prepare_query_node ì‹œì‘")
        
        # ì¿¼ë¦¬ í…ìŠ¤íŠ¸ êµ¬ì„±
        query_text = state.get("situation_text", "")
        if state.get("summary"):
            query_text = state["summary"]
            if state.get("details"):
                query_text = f"{state['summary']}\n\n{state['details']}"
        
        # ì„ë² ë”© ìƒì„±
        query_embedding = await self._get_embedding(query_text)
        
        return {
            **state,
            "query_text": query_text,
            "query_embedding": query_embedding,
        }
    
    async def classify_situation_node(self, state: SituationWorkflowState) -> SituationWorkflowState:
        """2. ìƒí™© ë¶„ë¥˜ (ì¹´í…Œê³ ë¦¬ + ìœ„í—˜ë„)"""
        logger.info("[ì›Œí¬í”Œë¡œìš°] classify_situation_node ì‹œì‘")
        
        query_text = state.get("query_text", "")
        category_hint = state.get("category_hint")
        
        # LLMìœ¼ë¡œ ë¶„ë¥˜ ìˆ˜í–‰
        classification = await self._llm_classify(
            situation_text=query_text,
            category_hint=category_hint,
            employment_type=state.get("employment_type"),
            work_period=state.get("work_period"),
            weekly_hours=state.get("weekly_hours"),
            is_probation=state.get("is_probation"),
            social_insurance=state.get("social_insurance"),
        )
        
        return {
            **state,
            "classification": classification,
        }
    
    async def filter_rules_node(self, state: SituationWorkflowState) -> SituationWorkflowState:
        """3. ë¶„ë¥˜ ê²°ê³¼ ê¸°ë°˜ ê·œì • í•„í„°ë§"""
        logger.info("[ì›Œí¬í”Œë¡œìš°] filter_rules_node ì‹œì‘")
        
        classification = state.get("classification", {})
        classified_type = classification.get("classified_type", "unknown")
        
        # ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ í•„í„°ë§ ê·œì¹™ ìƒì„±
        filtered_categories = await self._filter_rules_by_classification(
            classified_type=classified_type,
            classification=classification,
        )
        
        return {
            **state,
            "filtered_categories": filtered_categories,
        }
    
    async def retrieve_guides_node(self, state: SituationWorkflowState) -> SituationWorkflowState:
        """4. RAG ê²€ìƒ‰ (í•„í„°ë§ëœ ì¹´í…Œê³ ë¦¬ë§Œ) + legalBasis ì¶”ì¶œ"""
        logger.info("[ì›Œí¬í”Œë¡œìš°] retrieve_guides_node ì‹œì‘")
        
        query_embedding = state.get("query_embedding")
        filtered_categories = state.get("filtered_categories", [])
        
        if not query_embedding:
            logger.warning("[ì›Œí¬í”Œë¡œìš°] query_embeddingì´ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ê²°ê³¼ ë°˜í™˜")
            return {
                **state,
                "grounding_chunks": [],
                "related_cases": [],
                "legal_basis": [],
            }
        
        # ë³‘ë ¬ ê²€ìƒ‰
        grounding_chunks, related_cases = await asyncio.gather(
            self._search_legal_with_filter(
                query_embedding=query_embedding,
                categories=filtered_categories,
                top_k=8,
            ),
            self._search_cases_with_embedding(
                query_embedding=query_embedding,
                top_k=3,
            ),
            return_exceptions=False
        )
        
        # RAG ê²€ìƒ‰ ê²°ê³¼ ë¡œê¹…
        logger.info(f"[ì›Œí¬í”Œë¡œìš°] RAG ê²€ìƒ‰ ì™„ë£Œ: ë²•ë ¹/ê°€ì´ë“œ {len(grounding_chunks)}ê°œ, ì¼€ì´ìŠ¤ {len(related_cases)}ê°œ")
        if grounding_chunks:
            logger.info(f"[ì›Œí¬í”Œë¡œìš°] ê²€ìƒ‰ëœ ë²•ë ¹/ê°€ì´ë“œ ëª©ë¡:")
            for idx, chunk in enumerate(grounding_chunks[:5], 1):  # ìƒìœ„ 5ê°œë§Œ ë¡œê¹…
                logger.info(f"  {idx}. [{chunk.source_type}] {chunk.title} (score: {chunk.score:.3f})")
                logger.info(f"     ë‚´ìš©: {chunk.snippet[:100]}...")
        
        # legalBasis êµ¬ì¡° ì¶”ì¶œ (criteria ê°€ê³µìš©)
        legal_basis = self._extract_legal_basis(grounding_chunks)
        
        return {
            **state,
            "grounding_chunks": grounding_chunks,
            "related_cases": related_cases[:3],  # ìµœëŒ€ 3ê°œë§Œ
            "legal_basis": legal_basis,
        }
    
    async def generate_action_guide_node(self, state: SituationWorkflowState) -> SituationWorkflowState:
        """5. í–‰ë™ ê°€ì´ë“œ ìƒì„± (summary, criteria, actionPlan, scripts ëª¨ë‘ ìƒì„±)"""
        logger.info("[ì›Œí¬í”Œë¡œìš°] generate_action_guide_node ì‹œì‘")
        
        classification = state.get("classification", {})
        grounding_chunks = state.get("grounding_chunks", [])
        legal_basis = state.get("legal_basis", [])
        query_text = state.get("query_text", "")
        
        # legal_basisê°€ ë¹ˆ ë°°ì—´ì¼ ë•Œ fallback
        if not legal_basis:
            logger.warning("[ì›Œí¬í”Œë¡œìš°] legal_basisê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ criteria ìƒì„±")
            legal_basis = [{
                "title": "ë²•ì  ê·¼ê±° í™•ì¸ í•„ìš”",
                "snippet": "ê´€ë ¨ ë²•ë ¹ ì •ë³´ë¥¼ í™•ì¸í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.",
                "source_type": "unknown",
            }]
        
        # ì•¡ì…˜ ê°€ì´ë“œ ìƒì„± (summary í¬í•¨)
        action_result = await self._llm_generate_action_guide(
            situation_text=query_text,
            classification=classification,
            grounding_chunks=grounding_chunks,
            legal_basis=legal_basis,
            employment_type=state.get("employment_type"),
            work_period=state.get("work_period"),
            weekly_hours=state.get("weekly_hours"),
            is_probation=state.get("is_probation"),
            social_insurance=state.get("social_insurance"),
        )
        
        # ê²°ê³¼ ê²€ì¦ ë° ì •ê·œí™”
        logger.info(f"[ì›Œí¬í”Œë¡œìš°] _reformat_action_result í˜¸ì¶œ ì „ action_result: summary ê¸¸ì´={len(action_result.get('summary', ''))}, criteria ê°œìˆ˜={len(action_result.get('criteria', []))}")
        normalized_result = self._reformat_action_result(action_result, legal_basis)
        
        # action_plan ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        action_plan_safe = normalized_result.get('action_plan', {})
        if isinstance(action_plan_safe, dict):
            action_plan_steps_count = len(action_plan_safe.get('steps', []))
        else:
            action_plan_steps_count = 0
        
        logger.info(f"[ì›Œí¬í”Œë¡œìš°] _reformat_action_result í˜¸ì¶œ í›„ normalized_result: summary ê¸¸ì´={len(normalized_result.get('summary', ''))}, criteria ê°œìˆ˜={len(normalized_result.get('criteria', []))}, action_plan steps={action_plan_steps_count}")
        
        return {
            **state,
            "summary_report": normalized_result.get("summary", ""),  # 4ê°œ ì„¹ì…˜ ë§ˆí¬ë‹¤ìš´
            "action_plan": normalized_result.get("action_plan", {"steps": []}),  # steps êµ¬ì¡°
            "scripts": normalized_result.get("scripts", {}),  # toCompany, toAdvisor
            "criteria": normalized_result.get("criteria", []),  # name, status, reason
            "organizations": normalized_result.get("organizations", []),  # ì¶”ì²œ ê¸°ê´€ ëª©ë¡
        }
    
    
    async def merge_output_node(self, state: SituationWorkflowState) -> SituationWorkflowState:
        """7. ìµœì¢… ì¶œë ¥ ë³‘í•©"""
        logger.info("[ì›Œí¬í”Œë¡œìš°] merge_output_node ì‹œì‘")
        
        classification = state.get("classification", {})
        related_cases = state.get("related_cases", [])
        action_plan = state.get("action_plan", {})
        scripts = state.get("scripts", {})
        criteria = state.get("criteria", [])
        organizations = state.get("organizations", [])  # ì¶”ì²œ ê¸°ê´€ ëª©ë¡
        summary_report = state.get("summary_report", "")  # generate_action_guideì—ì„œ ìƒì„±ë¨
        
        # grounding_chunks ê°€ì ¸ì˜¤ê¸°
        grounding_chunks = state.get("grounding_chunks", [])
        
        # ìµœì¢… JSON ì¶œë ¥ êµ¬ì„±
        # related_casesëŠ” ì´ë¯¸ retrieve_guidesì—ì„œ ìµœëŒ€ 3ê°œë¡œ ì œí•œë¨
        # related_casesëŠ” dict í˜•íƒœë¡œ ë°˜í™˜ë˜ë¯€ë¡œ dict ì ‘ê·¼ ë°©ì‹ ì‚¬ìš©
        formatted_related_cases = []
        for case in related_cases[:3]:  # ìµœëŒ€ 3ê°œë§Œ (ì´ì¤‘ ì•ˆì „ì¥ì¹˜)
            if isinstance(case, dict):
                case_id = case.get("id", "")
                case_title = case.get("title", "")
                case_situation = case.get("situation", "")
                case_source_type = case.get("source_type")
            else:
                # ê°ì²´ì¸ ê²½ìš° (Legacy ì§€ì›)
                case_id = getattr(case, "id", "")
                case_title = getattr(case, "title", "")
                case_situation = getattr(case, "situation", "")
                case_source_type = getattr(case, "source_type", None)
            
            formatted_related_cases.append({
                "id": case_id,  # external_id
                "title": case_title,
                "summary": case_situation[:200] if len(case_situation) > 200 else case_situation,
                "source_type": case_source_type,  # source_type ì •ë³´ ì¶”ê°€
            })
        
        # grounding_chunksë¥¼ sources í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        formatted_sources = [
            {
                "source_id": chunk.source_id,
                "source_type": chunk.source_type,
                "title": chunk.title,
                "snippet": chunk.snippet,
                "score": chunk.score,
                "external_id": getattr(chunk, 'external_id', None),
                "file_url": getattr(chunk, 'file_url', None),
            }
            for chunk in grounding_chunks[:8]  # ìµœëŒ€ 8ê°œ
        ]
        
        # criteriaì— legalBasis ì¶”ê°€
        # ê° criterionì— ëŒ€í•´ ê´€ë ¨ëœ legalBasisë¥¼ ë§¤í•‘
        # í˜„ì¬ëŠ” ëª¨ë“  sourcesë¥¼ ê° criterionì— í• ë‹¹ (ë‚˜ì¤‘ì— LLMì´ criterionë³„ë¡œ ê´€ë ¨ sourcesë¥¼ ëª…ì‹œí•˜ë„ë¡ ê°œì„  ê°€ëŠ¥)
        criteria_with_legal_basis = []
        for criterion in criteria:
            criterion_dict = criterion.copy() if isinstance(criterion, dict) else {
                "name": getattr(criterion, "name", ""),
                "status": getattr(criterion, "status", "unclear"),
                "reason": getattr(criterion, "reason", ""),
            }
            
            # ê° criterionì— legalBasis ë°°ì—´ ì¶”ê°€
            # TODO: ë‚˜ì¤‘ì— LLMì´ criterionë³„ë¡œ ê´€ë ¨ sourcesë¥¼ ëª…ì‹œí•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ ê°œì„ 
            # í˜„ì¬ëŠ” ëª¨ë“  sourcesë¥¼ í• ë‹¹ (ë˜ëŠ” criterionì˜ name/reasonê³¼ ìœ ì‚¬í•œ sourcesë§Œ í•„í„°ë§)
            legal_basis_list = []
            for source in formatted_sources:
                legal_basis_item = {
                    "docId": source.get("source_id", ""),
                    "docTitle": source.get("title", ""),
                    "docType": source.get("source_type", "law"),
                    "snippet": source.get("snippet", ""),
                    "similarityScore": source.get("score", 0.0),
                    "externalId": source.get("external_id"),
                    "fileUrl": source.get("file_url"),
                }
                # chunk_indexê°€ ìˆìœ¼ë©´ ì¶”ê°€
                chunk_idx = None
                for chunk in grounding_chunks:
                    if chunk.source_id == source.get("source_id"):
                        chunk_idx = getattr(chunk, 'chunk_index', None)
                        break
                if chunk_idx is not None:
                    legal_basis_item["chunkIndex"] = chunk_idx
                
                legal_basis_list.append(legal_basis_item)
            
            criterion_dict["legalBasis"] = legal_basis_list
            criteria_with_legal_basis.append(criterion_dict)
        
        final_output = {
            "classified_type": classification.get("classified_type", "unknown"),
            "risk_score": classification.get("risk_score", 50),
            "summary": summary_report,  # generate_action_guideì—ì„œ ìƒì„±ëœ 4ê°œ ì„¹ì…˜ ë§ˆí¬ë‹¤ìš´
            "criteria": criteria_with_legal_basis,  # legalBasis í¬í•¨
            "action_plan": action_plan,  # steps êµ¬ì¡°
            "scripts": scripts,  # toCompany, toAdvisor
            "related_cases": formatted_related_cases,
            "grounding_chunks": formatted_sources,  # sources í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            "organizations": organizations,  # ì¶”ì²œ ê¸°ê´€ ëª©ë¡
        }
        
        return {
            **state,
            "final_output": final_output,
        }
    
    # ==================== ë‚´ë¶€ í—¬í¼ í•¨ìˆ˜ë“¤ ====================
    
    async def _get_embedding(self, text: str) -> List[float]:
        """ì„ë² ë”© ìƒì„± (ìºì‹± ì§€ì›)"""
        # legal_rag_serviceì˜ _get_embeddingê³¼ ë™ì¼í•œ ë¡œì§
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ generator ì‚¬ìš©
        return await asyncio.to_thread(self.generator.embed_one, text)
    
    async def _llm_classify(
        self,
        situation_text: str,
        category_hint: Optional[str] = None,
        employment_type: Optional[str] = None,
        work_period: Optional[str] = None,
        weekly_hours: Optional[int] = None,
        is_probation: Optional[bool] = None,
        social_insurance: Optional[str] = None,
    ) -> Dict[str, Any]:
        """LLMìœ¼ë¡œ ìƒí™© ë¶„ë¥˜"""
        # í”„ë¡¬í”„íŠ¸ ìƒì„± (ìƒˆë¡œ ë§Œë“¤ì–´ì•¼ í•¨)
        prompt = build_situation_classify_prompt(
            situation_text=situation_text,
            category_hint=category_hint,
            employment_type=employment_type,
            work_period=work_period,
            weekly_hours=weekly_hours,
            is_probation=is_probation,
            social_insurance=social_insurance,
        )
        
        # LLM í˜¸ì¶œ
        response = await self._call_llm(prompt)
        
        # JSON íŒŒì‹±
        import json
        import re
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            classification = json.loads(json_match.group())
            
            # classified_type ì •ê·œí™”: íŒŒì´í”„ë¡œ êµ¬ë¶„ëœ ê°’ì´ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ ê°’ë§Œ ì‚¬ìš©
            classified_type = classification.get("classified_type", category_hint or "unknown")
            if isinstance(classified_type, str) and "|" in classified_type:
                # íŒŒì´í”„ë¡œ êµ¬ë¶„ëœ ê²½ìš° ì²« ë²ˆì§¸ ê°’ë§Œ ì‚¬ìš©
                classified_type = classified_type.split("|")[0].strip()
                logger.warning(f"[ì›Œí¬í”Œë¡œìš°] classified_typeì— ì—¬ëŸ¬ ê°’ì´ í¬í•¨ë¨, ì²« ë²ˆì§¸ ê°’ë§Œ ì‚¬ìš©: {classification.get('classified_type')} -> {classified_type}")
            
            # ìœ íš¨í•œ ë¶„ë¥˜ ìœ í˜•ì¸ì§€ í™•ì¸
            valid_types = ["harassment", "unpaid_wage", "unfair_dismissal", "overtime", "probation", "unknown"]
            if classified_type not in valid_types:
                logger.warning(f"[ì›Œí¬í”Œë¡œìš°] ìœ íš¨í•˜ì§€ ì•Šì€ classified_type: {classified_type}, ê¸°ë³¸ê°’ìœ¼ë¡œ ë³€ê²½")
                classified_type = category_hint or "unknown"
            
            classification["classified_type"] = classified_type
            return classification
        
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
        return {
            "classified_type": category_hint or "unknown",
            "risk_score": 50,
            "categories": [],
        }
    
    async def _filter_rules_by_classification(
        self,
        classified_type: str,
        classification: Dict[str, Any],
    ) -> List[str]:
        """ë¶„ë¥˜ ê²°ê³¼ ê¸°ë°˜ ê·œì • í•„í„°ë§"""
        # classificationì—ì„œ categories ì¶”ì¶œ (LLMì´ ë°˜í™˜í•œ ê²½ìš°)
        llm_categories = classification.get("categories", [])
        
        # ì¹´í…Œê³ ë¦¬ ë§¤í•‘ (fallback)
        category_mapping = {
            "harassment": ["ì§ì¥ë‚´ê´´ë¡­í˜", "ëª¨ìš•", "ì¸ê²©ê¶Œ"],
            "unpaid_wage": ["ì„ê¸ˆì²´ë¶ˆ", "ìµœì €ì„ê¸ˆ", "ì„ê¸ˆì§€ê¸‰", "ì—°ì¥ê·¼ë¡œìˆ˜ë‹¹"],
            "unfair_dismissal": ["ë¶€ë‹¹í•´ê³ ", "ê³„ì•½í•´ì§€", "í•´ê³ í†µì§€"],
            "overtime": ["ì—°ì¥ê·¼ë¡œ", "ì•¼ê°„ê·¼ë¡œ", "íœ´ì¼ê·¼ë¡œ", "ê·¼ë¡œì‹œê°„"],
            "probation": ["ìˆ˜ìŠµ", "ì¸í„´", "ê³„ì•½ê¸°ê°„"],
            "unknown": [],
        }
        
        # LLMì´ ë°˜í™˜í•œ ì¹´í…Œê³ ë¦¬ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ë§¤í•‘ ì‚¬ìš©
        if llm_categories:
            return llm_categories
        
        return category_mapping.get(classified_type, [])
    
    async def _search_legal_with_filter(
        self,
        query_embedding: List[float],
        categories: List[str],
        top_k: int = 8,
    ) -> List[LegalGroundingChunk]:
        """ì¹´í…Œê³ ë¦¬ í•„í„°ë§ëœ ë²•ë ¹ ê²€ìƒ‰"""
        # í•„í„° êµ¬ì„±
        filters = None
        if categories:
            # metadataì— category í•„ë“œê°€ ìˆë‹¤ê³  ê°€ì •
            # ì‹¤ì œ êµ¬í˜„ì€ ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¡°ì— ë”°ë¼ ë‹¤ë¦„
            filters = {"category": categories}
        
        rows = self.vector_store.search_similar_legal_chunks(
            query_embedding=query_embedding,
            top_k=top_k,
            filters=filters,
        )
        
        results: List[LegalGroundingChunk] = []
        for r in rows:
            source_type = r.get("source_type", "law")
            title = r.get("title", "ì œëª© ì—†ìŒ")
            content = r.get("content", "")
            score = r.get("score", 0.0)
            results.append(
                LegalGroundingChunk(
                    source_id=r.get("id", ""),
                    source_type=source_type,
                    title=title,
                    snippet=content[:300],
                    score=score,
                )
            )
        return results
    
    async def _search_cases_with_embedding(
        self,
        query_embedding: List[float],
        top_k: int = 3,
    ) -> List[Dict[str, Any]]:
        """
        ì¼€ì´ìŠ¤ ê²€ìƒ‰ (case ë˜ëŠ” standard_contract íƒ€ì…)
        source_type ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ë°˜í™˜
        """
        # caseì™€ standard_contract ëª¨ë‘ ê²€ìƒ‰ (í•„í„° ì œê±°)
        rows = self.vector_store.search_similar_legal_chunks(
            query_embedding=query_embedding,
            top_k=top_k * 2,  # ë” ë§ì´ ê°€ì ¸ì™€ì„œ í•„í„°ë§
        )
        
        cases: List[Dict[str, Any]] = []
        for row in rows:
            source_type = row.get("source_type", "case")
            # case ë˜ëŠ” standard_contractë§Œ í¬í•¨
            if source_type not in ["case", "standard_contract"]:
                continue
            
            external_id = row.get("external_id", "")
            title = row.get("title", "ì œëª© ì—†ìŒ")
            content = row.get("content", "")
            metadata = row.get("metadata", {})
            
            cases.append({
                "id": external_id,
                "title": title,
                "situation": metadata.get("situation", content[:200]),
                "main_issues": metadata.get("issues", []),
                "source_type": source_type,  # source_type ì •ë³´ í¬í•¨
            })
            
            if len(cases) >= top_k:
                break
        
        return cases
    
    def _extract_legal_basis(self, grounding_chunks: List[LegalGroundingChunk]) -> List[Dict[str, Any]]:
        """RAG ê²€ìƒ‰ ê²°ê³¼ì—ì„œ legalBasis êµ¬ì¡° ì¶”ì¶œ"""
        legal_basis = []
        for chunk in grounding_chunks[:5]:  # ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©
            legal_basis.append({
                "title": chunk.title,
                "snippet": chunk.snippet,
                "source_type": chunk.source_type,
                "source_id": chunk.source_id,
            })
        return legal_basis
    
    async def _llm_generate_action_guide(
        self,
        situation_text: str,
        classification: Dict[str, Any],
        grounding_chunks: List[LegalGroundingChunk],
        legal_basis: List[Dict[str, Any]],
        employment_type: Optional[str] = None,
        work_period: Optional[str] = None,
        weekly_hours: Optional[int] = None,
        is_probation: Optional[bool] = None,
        social_insurance: Optional[str] = None,
    ) -> Dict[str, Any]:
        """í–‰ë™ ê°€ì´ë“œ ìƒì„± (summary, criteria, actionPlan, scripts ëª¨ë‘)"""
        prompt = build_situation_action_guide_prompt(
            situation_text=situation_text,
            classification=classification,
            grounding_chunks=grounding_chunks,
            legal_basis=legal_basis,
            employment_type=employment_type,
            work_period=work_period,
            weekly_hours=weekly_hours,
            is_probation=is_probation,
            social_insurance=social_insurance,
        )
        
        response = await self._call_llm(prompt)
        
        # JSON íŒŒì‹±
        import json
        import re
        
        # ì‘ë‹µ ë¡œê¹… (ë””ë²„ê¹…ìš©) - ê°•í™”
        logger.info(f"[ì›Œí¬í”Œë¡œìš°] LLM raw ì‘ë‹µ ê¸¸ì´: {len(response)}ì")
        logger.info(f"[ì›Œí¬í”Œë¡œìš°] LLM raw ì‘ë‹µ (ì²˜ìŒ 1500ì): {response[:1500]}")
        if len(response) > 1500:
            logger.info(f"[ì›Œí¬í”Œë¡œìš°] LLM raw ì‘ë‹µ (ë§ˆì§€ë§‰ 500ì): {response[-500:]}")
        
        # ì½”ë“œ ë¸”ë¡ ì œê±°
        response_clean = response.strip()
        if response_clean.startswith("```json"):
            response_clean = response_clean[7:]
        elif response_clean.startswith("```"):
            response_clean = response_clean[3:]
        if response_clean.endswith("```"):
            response_clean = response_clean[:-3]
        response_clean = response_clean.strip()
        
        # JSON ê°ì²´ ì¶”ì¶œ (ë” robustí•œ íŒŒì‹±)
        json_match = re.search(r'\{.*\}', response_clean, re.DOTALL)
        if not json_match:
            logger.error(f"[ì›Œí¬í”Œë¡œìš°] JSON ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. response_clean (ì²˜ìŒ 500ì): {response_clean[:500]}")
        
        if json_match:
            try:
                logger.debug(f"[ì›Œí¬í”Œë¡œìš°] JSON íŒŒì‹± ì‹œë„, response_clean ê¸¸ì´: {len(response_clean)}")
                # ì¤‘ê´„í˜¸ ë§¤ì¹­ìœ¼ë¡œ ìœ íš¨í•œ JSON ì¶”ì¶œ
                json_str = json_match.group()
                brace_count = 0
                last_valid_pos = -1
                for i, char in enumerate(json_str):
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            last_valid_pos = i + 1
                            break
                
                if last_valid_pos > 0:
                    json_str = json_str[:last_valid_pos]
                
                # summary í•„ë“œì˜ ì œì–´ ë¬¸ì ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
                # JSON ë¬¸ìì—´ ë‚´ì—ì„œ ì œì–´ ë¬¸ì(ê°œí–‰, íƒ­ ë“±)ë¥¼ ì´ìŠ¤ì¼€ì´í”„
                def escape_control_chars_in_json_string(json_str: str) -> str:
                    """JSON ë¬¸ìì—´ ë‚´ì˜ ì œì–´ ë¬¸ìë¥¼ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬"""
                    # summary í•„ë“œ ì°¾ê¸°
                    summary_pattern = r'"summary"\s*:\s*"([^"]*(?:\\.[^"]*)*)"'
                    
                    def escape_summary(match):
                        field_name = match.group(0).split(':')[0]  # "summary"
                        value = match.group(1)  # ì‹¤ì œ ê°’
                        
                        # ì´ë¯¸ ì´ìŠ¤ì¼€ì´í”„ëœ ë¬¸ìëŠ” ìœ ì§€í•˜ë©´ì„œ ì œì–´ ë¬¸ì ì´ìŠ¤ì¼€ì´í”„
                        # ê°œí–‰ ë¬¸ì ì²˜ë¦¬
                        value = value.replace('\n', '\\n').replace('\r', '\\r').replace('\t', '\\t')
                        # ë”°ì˜´í‘œ ì´ìŠ¤ì¼€ì´í”„ (ì´ë¯¸ ì´ìŠ¤ì¼€ì´í”„ëœ ê²ƒì€ ì œì™¸)
                        value = re.sub(r'(?<!\\)"', '\\"', value)
                        
                        return f'{field_name}: "{value}"'
                    
                    # summary í•„ë“œë§Œ ì²˜ë¦¬
                    json_str = re.sub(summary_pattern, escape_summary, json_str, flags=re.DOTALL)
                    return json_str
                
                # summary í•„ë“œì˜ ì œì–´ ë¬¸ì ì²˜ë¦¬ (legal_rag_serviceì™€ ë™ì¼í•œ ë¡œì§)
                def clean_summary_field_in_json(json_str: str) -> str:
                    """summary í•„ë“œ ë‚´ë¶€ì˜ ì œì–´ ë¬¸ìë¥¼ JSON ì´ìŠ¤ì¼€ì´í”„ë¡œ ë³€í™˜"""
                    try:
                        # summary í•„ë“œì˜ ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸°
                        summary_start = json_str.find('"summary"')
                        if summary_start == -1:
                            return json_str
                        
                        # summary í•„ë“œì˜ ê°’ ì‹œì‘ ìœ„ì¹˜ ì°¾ê¸° (ì½œë¡ ê³¼ ë”°ì˜´í‘œ ì´í›„)
                        value_start = json_str.find('"', summary_start + 9)  # "summary" ê¸¸ì´ + 1
                        if value_start == -1:
                            return json_str
                        
                        value_start += 1  # ë”°ì˜´í‘œ ë‹¤ìŒë¶€í„°
                        
                        # ë¬¸ìì—´ ë ì°¾ê¸° (ì´ìŠ¤ì¼€ì´í”„ëœ ë”°ì˜´í‘œ ê³ ë ¤)
                        value_end = value_start
                        while value_end < len(json_str):
                            char = json_str[value_end]
                            
                            # ì´ìŠ¤ì¼€ì´í”„ëœ ë¬¸ì ê±´ë„ˆë›°ê¸°
                            if char == '\\' and value_end + 1 < len(json_str):
                                value_end += 2
                                continue
                            
                            # ë”°ì˜´í‘œ ì²˜ë¦¬
                            if char == '"':
                                # ì•ì˜ ë°±ìŠ¬ë˜ì‹œ ê°œìˆ˜ ì„¸ê¸°
                                backslash_count = 0
                                i = value_end - 1
                                while i >= value_start and json_str[i] == '\\':
                                    backslash_count += 1
                                    i -= 1
                                # í™€ìˆ˜ ê°œì˜ ë°±ìŠ¬ë˜ì‹œë©´ ì´ìŠ¤ì¼€ì´í”„ëœ ë”°ì˜´í‘œ, ì§ìˆ˜ ê°œë©´ ë¬¸ìì—´ ë
                                if backslash_count % 2 == 0:
                                    break
                            
                            value_end += 1
                        
                        if value_end >= len(json_str):
                            # ë¬¸ìì—´ ëì„ ì°¾ì§€ ëª»í•œ ê²½ìš°, ë‹¤ìŒ í°ë”°ì˜´í‘œê¹Œì§€ ì°¾ê¸°
                            next_quote = json_str.find('"', value_start)
                            if next_quote > value_start:
                                value_end = next_quote
                            else:
                                return json_str
                        
                        # summary í•„ë“œ ë‚´ìš© ì¶”ì¶œ
                        content = json_str[value_start:value_end]
                        
                        # ì´ìŠ¤ì¼€ì´í”„ëœ ë¬¸ìë¥¼ ì‹¤ì œ ë¬¸ìë¡œ ë³€í™˜ (ì¼ì‹œì )
                        content_decoded = content.replace('\\n', '\n').replace('\\r', '\r').replace('\\t', '\t').replace('\\"', '"').replace('\\\\', '\\')
                        
                        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
                        content_decoded = re.sub(r'```markdown\s*', '', content_decoded, flags=re.IGNORECASE)
                        content_decoded = re.sub(r'```\s*', '', content_decoded, flags=re.MULTILINE)
                        
                        # ì œì–´ ë¬¸ìë¥¼ JSON ì´ìŠ¤ì¼€ì´í”„ë¡œ ë³€í™˜
                        result = []
                        for char in content_decoded:
                            if char == '\n':
                                result.append('\\n')
                            elif char == '\r':
                                result.append('\\r')
                            elif char == '\t':
                                result.append('\\t')
                            elif char == '"':
                                result.append('\\"')
                            elif char == '\\':
                                result.append('\\\\')
                            elif ord(char) < 32:  # ì œì–´ ë¬¸ì
                                result.append(f'\\u{ord(char):04x}')
                            else:
                                result.append(char)
                        
                        # summary í•„ë“œ êµì²´
                        cleaned_content = ''.join(result)
                        return json_str[:value_start] + cleaned_content + json_str[value_end:]
                    except Exception as e:
                        logger.warning(f"[ì›Œí¬í”Œë¡œìš°] summary í•„ë“œ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}, ì›ë³¸ JSON ì‚¬ìš©")
                        return json_str
                
                # summary í•„ë“œ ì •ë¦¬
                json_str_cleaned = clean_summary_field_in_json(json_str)
                
                # JSON íŒŒì‹±
                result = json.loads(json_str_cleaned)
                
                # summary í•„ë“œì—ì„œ ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±° (ìˆëŠ” ê²½ìš°)
                if "summary" in result and isinstance(result["summary"], str):
                    summary = result["summary"]
                    # ```markdown ... ``` ì œê±°
                    summary = re.sub(r'```markdown\s*', '', summary, flags=re.IGNORECASE)
                    summary = re.sub(r'```\s*$', '', summary, flags=re.MULTILINE)
                    # ë”°ì˜´í‘œ escape ì²˜ë¦¬ (JSON ë‚´ë¶€ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë¨)
                    
                    # í•œì/ì¼ë³¸ì–´ ë¬¸ìë¥¼ í•œê¸€ë¡œ ë³€í™˜ ë˜ëŠ” ì œê±°
                    def remove_cjk_japanese(text: str) -> str:
                        """í•œì, ì¼ë³¸ì–´ ë¬¸ìë¥¼ ì œê±°í•˜ê±°ë‚˜ í•œê¸€ë¡œ ë³€í™˜"""
                        import unicodedata
                        
                        # ì¼ë°˜ì ì¸ í•œì-í•œê¸€ ë§¤í•‘
                        hanja_to_hangul = {
                            'æœ€è¿‘': 'ìµœê·¼',
                            'å…¸å‹': 'ì „í˜•',
                            'å…¸å‹ì ì¸': 'ì „í˜•ì ì¸',
                        }
                        
                        # ë§¤í•‘ëœ í•œì ë³€í™˜
                        for hanja, hangul in hanja_to_hangul.items():
                            text = text.replace(hanja, hangul)
                        
                        # í•œì ë²”ìœ„ (CJK í†µí•© í•œì: U+4E00â€“U+9FFF, í•œì ë³´ì¶©: U+3400â€“U+4DBF)
                        # ì¼ë³¸ì–´ íˆë¼ê°€ë‚˜: U+3040â€“U+309F, ê°€íƒ€ì¹´ë‚˜: U+30A0â€“U+30FF
                        result = []
                        for char in text:
                            code = ord(char)
                            # í•œì ë²”ìœ„ ì²´í¬
                            is_hanja = (0x4E00 <= code <= 0x9FFF) or (0x3400 <= code <= 0x4DBF)
                            # ì¼ë³¸ì–´ ë²”ìœ„ ì²´í¬
                            is_japanese = (0x3040 <= code <= 0x309F) or (0x30A0 <= code <= 0x30FF)
                            
                            if is_hanja or is_japanese:
                                # í•œì/ì¼ë³¸ì–´ ë¬¸ìëŠ” ì œê±°
                                logger.debug(f"[ì›Œí¬í”Œë¡œìš°] í•œì/ì¼ë³¸ì–´ ë¬¸ì ì œê±°: {char} (U+{code:04X})")
                                continue
                            result.append(char)
                        
                        return ''.join(result)
                    
                    summary = remove_cjk_japanese(summary)
                    
                    result["summary"] = summary.strip()
                
                logger.info("[ì›Œí¬í”Œë¡œìš°] JSON íŒŒì‹± ì„±ê³µ")
                
                # action_plan ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                action_plan_safe = result.get('action_plan', {})
                if isinstance(action_plan_safe, dict):
                    action_plan_steps_count = len(action_plan_safe.get('steps', []))
                else:
                    action_plan_steps_count = 0
                
                logger.info(f"[ì›Œí¬í”Œë¡œìš°] íŒŒì‹±ëœ action_result: summary ê¸¸ì´={len(result.get('summary', ''))}, criteria ê°œìˆ˜={len(result.get('criteria', []))}, action_plan steps={action_plan_steps_count}")
                return result
            except json.JSONDecodeError as e:
                logger.error(f"[ì›Œí¬í”Œë¡œìš°] JSON íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                if hasattr(e, 'lineno') and hasattr(e, 'colno'):
                    logger.error(f"[ì›Œí¬í”Œë¡œìš°] ì—ëŸ¬ ìœ„ì¹˜: line {e.lineno}, column {e.colno}")
                logger.error(f"[ì›Œí¬í”Œë¡œìš°] ì‘ë‹µ ì›ë¬¸ (ì²˜ìŒ 1000ì): {response_clean[:1000]}")
                if 'json_str_cleaned' in locals():
                    logger.error(f"[ì›Œí¬í”Œë¡œìš°] json_str_cleaned ê¸¸ì´: {len(json_str_cleaned)}")
                    logger.error(f"[ì›Œí¬í”Œë¡œìš°] json_str_cleaned (ì²˜ìŒ 500ì): {json_str_cleaned[:500]}")
                
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œì—ë„ ë¶€ë¶„ì ìœ¼ë¡œ íŒŒì‹± ì‹œë„
                try:
                    # summary, criteria, action_plan, scripts í•„ë“œ ì¶”ì¶œ ì‹œë„
                    json_to_search = json_str_cleaned if 'json_str_cleaned' in locals() else (json_str if 'json_str' in locals() else response_clean)
                    
                    # ì—¬ëŸ¬ íŒ¨í„´ìœ¼ë¡œ summary í•„ë“œ ì°¾ê¸°
                    summary_patterns = [
                        r'"summary"\s*:\s*"((?:[^"\\]|\\.)*)"',  # ì¼ë°˜ì ì¸ JSON ë¬¸ìì—´
                        r'"summary"\s*:\s*"([^"]*)"',  # ê°„ë‹¨í•œ íŒ¨í„´
                        r'summary["\s]*:["\s]*([^",}]+)',  # ë” ìœ ì—°í•œ íŒ¨í„´
                    ]
                    
                    summary_text = None
                    for pattern in summary_patterns:
                        summary_match = re.search(pattern, json_to_search, re.DOTALL | re.IGNORECASE)
                        if summary_match:
                            summary_text = summary_match.group(1)
                            # ì´ìŠ¤ì¼€ì´í”„ ì œê±°
                            summary_text = summary_text.replace('\\n', '\n').replace('\\r', '\r').replace('\\t', '\t').replace('\\"', '"')
                            logger.warning(f"[ì›Œí¬í”Œë¡œìš°] summary í•„ë“œ ì¶”ì¶œ ì„±ê³µ (íŒ¨í„´: {pattern[:30]}...)")
                            break
                    
                    # criteria í•„ë“œ ì¶”ì¶œ ì‹œë„
                    criteria_list = []
                    try:
                        # criteria ë°°ì—´ íŒ¨í„´ ì°¾ê¸° (ë” ê°•ë ¥í•œ íŒ¨í„´)
                        # ë¨¼ì € criteria ë°°ì—´ì˜ ì‹œì‘ê³¼ ëì„ ì°¾ê¸°
                        criteria_start = json_to_search.find('"criteria"')
                        if criteria_start != -1:
                            # criteria ë‹¤ìŒì˜ [ ì°¾ê¸°
                            bracket_start = json_to_search.find('[', criteria_start)
                            if bracket_start != -1:
                                # ì¤‘ì²©ëœ ì¤‘ê´„í˜¸ì™€ ëŒ€ê´„í˜¸ë¥¼ ê³ ë ¤í•˜ì—¬ ë°°ì—´ ë ì°¾ê¸°
                                bracket_count = 0
                                bracket_end = bracket_start
                                in_string = False
                                escape_next = False
                                
                                for i in range(bracket_start, len(json_to_search)):
                                    char = json_to_search[i]
                                    
                                    if escape_next:
                                        escape_next = False
                                        continue
                                    
                                    if char == '\\':
                                        escape_next = True
                                        continue
                                    
                                    if char == '"' and not escape_next:
                                        in_string = not in_string
                                        continue
                                    
                                    if not in_string:
                                        if char == '[':
                                            bracket_count += 1
                                        elif char == ']':
                                            bracket_count -= 1
                                            if bracket_count == 0:
                                                bracket_end = i + 1
                                                break
                                
                                if bracket_end > bracket_start:
                                    criteria_array_str = json_to_search[bracket_start:bracket_end]
                                    # ê° ê°ì²´ ì¶”ì¶œ (ê°„ë‹¨í•œ íŒ¨í„´)
                                    # { "name": "...", "status": "...", "reason": "..." }
                                    item_pattern = r'\{\s*"name"\s*:\s*"((?:[^"\\]|\\.)*)"\s*,\s*"status"\s*:\s*"((?:[^"\\]|\\.)*)"\s*,\s*"reason"\s*:\s*"((?:[^"\\]|\\.)*)"'
                                    items = re.findall(item_pattern, criteria_array_str, re.DOTALL)
                                    for name, status, reason in items:
                                        # ì´ìŠ¤ì¼€ì´í”„ ì œê±°
                                        name = name.replace('\\"', '"').replace('\\n', '\n').replace('\\r', '\r').replace('\\t', '\t')
                                        status = status.replace('\\"', '"').replace('\\n', '\n').replace('\\r', '\r').replace('\\t', '\t')
                                        reason = reason.replace('\\"', '"').replace('\\n', '\n').replace('\\r', '\r').replace('\\t', '\t')
                                        criteria_list.append({
                                            "name": name,
                                            "status": status,
                                            "reason": reason,
                                        })
                                    if criteria_list:
                                        logger.warning(f"[ì›Œí¬í”Œë¡œìš°] criteria í•„ë“œ ì¶”ì¶œ ì„±ê³µ: {len(criteria_list)}ê°œ")
                    except Exception as criteria_error:
                        logger.warning(f"[ì›Œí¬í”Œë¡œìš°] criteria ì¶”ì¶œ ì‹¤íŒ¨: {str(criteria_error)}")
                    
                    if summary_text:
                        # ë¶€ë¶„ íŒŒì‹± ê²°ê³¼ ë°˜í™˜ (criteria í¬í•¨)
                        return {
                            "summary": summary_text.strip(),
                            "action_plan": {"steps": []},
                            "scripts": {},
                            "criteria": criteria_list,  # ì¶”ì¶œëœ criteria ì‚¬ìš©
                        }
                    else:
                        logger.warning("[ì›Œí¬í”Œë¡œìš°] summary í•„ë“œë„ ì¶”ì¶œ ì‹¤íŒ¨")
                except Exception as partial_error:
                    logger.warning(f"[ì›Œí¬í”Œë¡œìš°] ë¶€ë¶„ íŒŒì‹±ë„ ì‹¤íŒ¨: {str(partial_error)}", exc_info=True)
        
        # ê¸°ë³¸ê°’ (4ê°œ ì„¹ì…˜ êµ¬ì¡° ìœ ì§€)
        logger.warning("[ì›Œí¬í”Œë¡œìš°] JSON íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ë°˜í™˜")
        if 'response_clean' in locals():
            logger.warning(f"[ì›Œí¬í”Œë¡œìš°] response_clean (ì²˜ìŒ 500ì): {response_clean[:500]}")
        return {
            "summary": "## ğŸ“Š ìƒí™© ë¶„ì„ì˜ ê²°ê³¼\n\në¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\n## âš–ï¸ ë²•ì  ê´€ì ì—ì„œ ë³¸ í˜„ì¬ìƒí™©\n\në²•ì  ê·¼ê±°ë¥¼ í™•ì¸í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\n## ğŸ¯ ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™\n\n- ìƒí™©ì„ ë‹¤ì‹œ ì…ë ¥í•´ ì£¼ì„¸ìš”\n- ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”\n\n## ğŸ’¬ ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”\n\nìƒë‹´ ê¸°ê´€ì— ë¬¸ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.",
            "action_plan": {"steps": []},
            "scripts": {},
            "criteria": [],
        }
    
    def _reconstruct_summary_sections(self, summary: str, section_patterns: List[Dict[str, Any]], classified_type: str = "unknown") -> Optional[str]:
        """LLMì´ ìƒì„±í•œ summaryë¥¼ íŒŒì‹±í•˜ì—¬ ì˜¬ë°”ë¥¸ ì„¹ì…˜ í˜•ì‹ìœ¼ë¡œ ì¬êµ¬ì„±"""
        try:
            # summaryë¥¼ ì¤„ ë‹¨ìœ„ë¡œ ë¶„í• 
            lines = summary.split('\n')
            reconstructed_parts = []
            
            # ê° ì„¹ì…˜ë³„ë¡œ ë‚´ìš© ì¶”ì¶œ
            section_contents = {}
            current_section_key = None
            
            for i, line in enumerate(lines):
                line_stripped = line.strip()
                
                # ì„¹ì…˜ í—¤ë” ì°¾ê¸° (í‚¤ì›Œë“œ ê¸°ë°˜)
                for section_info in section_patterns:
                    for keyword in section_info["keywords"]:
                        # í—¤ë” í˜•ì‹ í™•ì¸ (## í‚¤ì›Œë“œ, # í‚¤ì›Œë“œ, ë˜ëŠ” í‚¤ì›Œë“œë§Œ)
                        if re.match(rf'^##?\s*.*?{re.escape(keyword)}', line_stripped, re.IGNORECASE) or \
                           (line_stripped and keyword in line_stripped and len(line_stripped) < 50):
                            current_section_key = section_info["title"]
                            if current_section_key not in section_contents:
                                section_contents[current_section_key] = []
                            break
                    if current_section_key:
                        break
                
                # í˜„ì¬ ì„¹ì…˜ì— ë‚´ìš© ì¶”ê°€
                if current_section_key:
                    # í—¤ë” ë¼ì¸ì´ ì•„ë‹ˆë©´ ë‚´ìš©ìœ¼ë¡œ ì¶”ê°€
                    is_header = False
                    for section_info in section_patterns:
                        for keyword in section_info["keywords"]:
                            if re.match(rf'^##?\s*.*?{re.escape(keyword)}', line_stripped, re.IGNORECASE):
                                is_header = True
                                break
                        if is_header:
                            break
                    
                    if not is_header:
                        section_contents[current_section_key].append(line)
                else:
                    # ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì„¹ì…˜ìœ¼ë¡œ ê°„ì£¼
                    if not section_contents:
                        first_section = section_patterns[0]["title"]
                        section_contents[first_section] = [line]
            
            # ì¬êµ¬ì„±ëœ summary ìƒì„±
            for section_info in section_patterns:
                title = section_info["title"]
                if title in section_contents and section_contents[title]:
                    reconstructed_parts.append(title)
                    reconstructed_parts.append("")
                    reconstructed_parts.extend(section_contents[title])
                    reconstructed_parts.append("")
                else:
                    # ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ì¹´í…Œê³ ë¦¬ë³„ ê¸°ë³¸ ë©”ì‹œì§€ ì¶”ê°€
                    emoji = section_info.get("emoji", "")
                    
                    default_content_by_type = {
                        "unpaid_wage": {
                            "âš–ï¸": "ì„ê¸ˆì²´ë¶ˆì€ ê·¼ë¡œê¸°ì¤€ë²• ì œ43ì¡°(ì„ê¸ˆì§€ê¸‰), ì œ36ì¡°(ì„ê¸ˆì˜ ì§€ê¸‰)ì™€ ê´€ë ¨ëœ ì‚¬ì•ˆì…ë‹ˆë‹¤. ì‚¬ìš©ìëŠ” ê·¼ë¡œìì—ê²Œ ì„ê¸ˆì„ ì •ê¸°ì ìœ¼ë¡œ ì§€ê¸‰í•  ì˜ë¬´ê°€ ìˆìœ¼ë©°, ì´ë¥¼ ìœ„ë°˜í•  ê²½ìš° í˜•ì‚¬ì²˜ë²Œê³¼ ë¯¼ì‚¬ìƒ ì†í•´ë°°ìƒ ì±…ì„ì„ ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                            "ğŸ¯": "- ê·¼ë¡œê³„ì•½ì„œì™€ ê¸‰ì—¬ëª…ì„¸ì„œë¥¼ í™•ì¸í•˜ì„¸ìš”\n- ì¶œí‡´ê·¼ ê¸°ë¡ê³¼ ê·¼ë¬´ì‹œê°„ì„ ì •ë¦¬í•˜ì„¸ìš”\n- ì„ê¸ˆ ì§€ê¸‰ ë‚´ì—­ì„ ë¬¸ì„œë¡œ ë³´ê´€í•˜ì„¸ìš”",
                            "ğŸ’¬": "íšŒì‚¬ì— ì •ì¤‘í•˜ê²Œ ì„ê¸ˆ ì§€ê¸‰ì„ ìš”ì²­í•˜ëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                        },
                        "harassment": {
                            "âš–ï¸": "ì§ì¥ ë‚´ ê´´ë¡­í˜ì€ ì§ì¥ ë‚´ ê´´ë¡­í˜ ë°©ì§€ ë° ê·¼ë¡œì ë³´í˜¸ ë“±ì— ê´€í•œ ë²•ë¥ ì— ë”°ë¼ ê¸ˆì§€ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì—…ë¬´ìƒ ì§€ìœ„ë‚˜ ê´€ê³„ë¥¼ ì´ìš©í•˜ì—¬ ê·¼ë¡œìì—ê²Œ ì‹ ì²´ì Â·ì •ì‹ ì  ê³ í†µì„ ì£¼ëŠ” í–‰ìœ„ëŠ” ë²•ì  ì²˜ë²Œ ëŒ€ìƒì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                            "ğŸ¯": "- ê´´ë¡­í˜ ê´€ë ¨ ì¦ê±° ìë£Œë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”\n- ëŒ€í™” ë‚´ìš©ê³¼ ì¼ì‹œë¥¼ ê¸°ë¡í•˜ì„¸ìš”\n- ìƒí™©ì„ ê°ê´€ì ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”",
                            "ğŸ’¬": "íšŒì‚¬ì— ê´´ë¡­í˜ ìƒí™©ì„ ì •ì¤‘í•˜ê²Œ ì•Œë¦¬ëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                        },
                        "unfair_dismissal": {
                            "âš–ï¸": "ë¶€ë‹¹í•´ê³ ëŠ” ê·¼ë¡œê¸°ì¤€ë²• ì œ23ì¡°(í•´ê³ ì˜ ì œí•œ)ì— ë”°ë¼ ì œí•œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì •ë‹¹í•œ ì‚¬ìœ  ì—†ì´ í•´ê³ í•˜ëŠ” ê²½ìš° ë³µì§ ì²­êµ¬ë‚˜ ì†í•´ë°°ìƒ ì²­êµ¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
                            "ğŸ¯": "- í•´ê³  í†µì§€ì„œì™€ ê´€ë ¨ ë¬¸ì„œë¥¼ ë³´ê´€í•˜ì„¸ìš”\n- ê·¼ë¬´ ê¸°ê°„ê³¼ ì„±ê³¼ë¥¼ ì •ë¦¬í•˜ì„¸ìš”\n- íšŒì‚¬ì™€ì˜ ëŒ€í™” ë‚´ìš©ì„ ê¸°ë¡í•˜ì„¸ìš”",
                            "ğŸ’¬": "íšŒì‚¬ì— í•´ê³  ì‚¬ìœ ì— ëŒ€í•œ ì„¤ëª…ì„ ìš”ì²­í•˜ëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                        },
                        "overtime": {
                            "âš–ï¸": "ê·¼ë¡œì‹œê°„ì€ ê·¼ë¡œê¸°ì¤€ë²• ì œ50ì¡°(ê·¼ë¡œì‹œê°„), ì œ53ì¡°(ì—°ì¥ê·¼ë¡œ)ì— ë”°ë¼ ê·œì œë©ë‹ˆë‹¤. ë²•ì • ê·¼ë¡œì‹œê°„ì„ ì´ˆê³¼í•˜ëŠ” ì—°ì¥ê·¼ë¡œì— ëŒ€í•´ì„œëŠ” ê°€ì‚°ì„ê¸ˆì„ ì§€ê¸‰í•´ì•¼ í•©ë‹ˆë‹¤.",
                            "ğŸ¯": "- ê·¼ë¬´ì‹œê°„ ê¸°ë¡ì„ í™•ì¸í•˜ì„¸ìš”\n- ì—°ì¥ê·¼ë¡œ ì‹œê°„ì„ ê³„ì‚°í•˜ì„¸ìš”\n- íœ´ê²Œì‹œê°„ ì¤€ìˆ˜ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”",
                            "ğŸ’¬": "íšŒì‚¬ì— ê·¼ë¡œì‹œê°„ê³¼ ê°€ì‚°ì„ê¸ˆì— ëŒ€í•´ ë¬¸ì˜í•˜ëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                        },
                        "probation": {
                            "âš–ï¸": "ìˆ˜ìŠµê¸°ê°„ì€ ê·¼ë¡œê¸°ì¤€ë²•ì— ë”°ë¼ í•©ë¦¬ì ì¸ ë²”ìœ„ ë‚´ì—ì„œë§Œ ì¸ì •ë©ë‹ˆë‹¤. ìˆ˜ìŠµê¸°ê°„ ì¤‘ì—ë„ ê·¼ë¡œê¸°ì¤€ë²•ìƒ ë³´í˜¸ë¥¼ ë°›ìœ¼ë©°, ë¶€ë‹¹í•œ í•´ê³ ëŠ” ì œí•œë©ë‹ˆë‹¤.",
                            "ğŸ¯": "- ìˆ˜ìŠµ ê¸°ê°„ê³¼ ì¡°ê±´ì„ í™•ì¸í•˜ì„¸ìš”\n- ê·¼ë¡œê³„ì•½ì„œì˜ ìˆ˜ìŠµ ì¡°í•­ì„ ê²€í† í•˜ì„¸ìš”\n- ìˆ˜ìŠµ ê¸°ê°„ ì¤‘ í‰ê°€ ë‚´ìš©ì„ ì •ë¦¬í•˜ì„¸ìš”",
                            "ğŸ’¬": "íšŒì‚¬ì— ìˆ˜ìŠµê¸°ê°„ê³¼ í‰ê°€ ê¸°ì¤€ì— ëŒ€í•´ ë¬¸ì˜í•˜ëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                        },
                    }
                    
                    default_content = default_content_by_type.get(classified_type, {
                        "âš–ï¸": "ê´€ë ¨ ë²•ë ¹ì„ í™•ì¸í•˜ì—¬ í˜„ì¬ ìƒí™©ì„ ë²•ì ìœ¼ë¡œ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤.",
                        "ğŸ¯": "- ìƒí™©ì„ ê°ê´€ì ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”\n- ê´€ë ¨ ë¬¸ì„œë¥¼ ë³´ê´€í•˜ì„¸ìš”\n- ì¦ê±° ìë£Œë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”",
                        "ğŸ’¬": "íšŒì‚¬ë‚˜ ìƒë‹´ ê¸°ê´€ì— ìƒí™©ì„ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                    })
                    
                    default_text = default_content.get(emoji, "í•´ë‹¹ ì„¹ì…˜ ë‚´ìš©ì„ í™•ì¸í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.")
                    reconstructed_parts.append(title)
                    reconstructed_parts.append("")
                    reconstructed_parts.append(default_text)
                    reconstructed_parts.append("")
            
            return '\n'.join(reconstructed_parts).strip()
        except Exception as e:
            logger.warning(f"[ì›Œí¬í”Œë¡œìš°] summary ì„¹ì…˜ ì¬êµ¬ì„± ì‹¤íŒ¨: {str(e)}")
            return None
    
    def _remove_cjk_japanese(self, text: str) -> str:
        """í•œì, ì¼ë³¸ì–´ ë¬¸ìë¥¼ ì œê±°í•˜ê±°ë‚˜ í•œê¸€ë¡œ ë³€í™˜ (ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜)"""
        if not isinstance(text, str):
            return text
        
        # ì¼ë°˜ì ì¸ í•œì-í•œê¸€ ë§¤í•‘
        hanja_to_hangul = {
            'æœ€è¿‘': 'ìµœê·¼',
            'å…¸å‹': 'ì „í˜•',
            'å…¸å‹ì ì¸': 'ì „í˜•ì ì¸',
        }
        
        # ë§¤í•‘ëœ í•œì ë³€í™˜
        for hanja, hangul in hanja_to_hangul.items():
            text = text.replace(hanja, hangul)
        
        # í•œì ë²”ìœ„ (CJK í†µí•© í•œì: U+4E00â€“U+9FFF, í•œì ë³´ì¶©: U+3400â€“U+4DBF)
        # ì¼ë³¸ì–´ íˆë¼ê°€ë‚˜: U+3040â€“U+309F, ê°€íƒ€ì¹´ë‚˜: U+30A0â€“U+30FF
        result = []
        for char in text:
            code = ord(char)
            # í•œì ë²”ìœ„ ì²´í¬
            is_hanja = (0x4E00 <= code <= 0x9FFF) or (0x3400 <= code <= 0x4DBF)
            # ì¼ë³¸ì–´ ë²”ìœ„ ì²´í¬
            is_japanese = (0x3040 <= code <= 0x309F) or (0x30A0 <= code <= 0x30FF)
            
            if is_hanja or is_japanese:
                # í•œì/ì¼ë³¸ì–´ ë¬¸ìëŠ” ì œê±°
                logger.debug(f"[ì›Œí¬í”Œë¡œìš°] í•œì/ì¼ë³¸ì–´ ë¬¸ì ì œê±°: {char} (U+{code:04X})")
                continue
            result.append(char)
        
        return ''.join(result)
    
    def _reformat_action_result(self, action_result: Dict[str, Any], legal_basis: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """ì•¡ì…˜ ê²°ê³¼ ì •ê·œí™” ë° ê²€ì¦"""
        import json
        import re
        
        result = action_result.copy()
        
        # 1. criteria ê²€ì¦ ë° fallback
        criteria = result.get("criteria", [])
        if not criteria or len(criteria) == 0:
            logger.warning("[ì›Œí¬í”Œë¡œìš°] criteriaê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. legal_basis ê¸°ë°˜ fallback ì‹œë„")
            # legal_basis ê¸°ë°˜ fallback
            if legal_basis and len(legal_basis) > 0:
                criteria = []
                for basis in legal_basis[:3]:
                    criteria.append({
                        "name": basis.get("title", "ë²•ì  ê·¼ê±°"),
                        "status": "unclear",
                        "reason": basis.get("snippet", "")[:300] if basis.get("snippet") else "ê´€ë ¨ ë²•ë ¹ ì •ë³´ë¥¼ í™•ì¸í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.",
                    })
                logger.info(f"[ì›Œí¬í”Œë¡œìš°] legal_basis ê¸°ë°˜ criteria ìƒì„±: {len(criteria)}ê°œ")
            else:
                criteria = [{
                    "name": "ë²•ì  ê·¼ê±° í™•ì¸ í•„ìš”",
                    "status": "unclear",
                    "reason": "ê´€ë ¨ ë²•ë ¹ ì •ë³´ë¥¼ í™•ì¸í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤."
                }]
        else:
            # criteria êµ¬ì¡° ê²€ì¦ ë° í•„í„°ë§
            validated_criteria = []
            for item in criteria:
                if not isinstance(item, dict):
                    continue
                
                name = item.get("name", "").strip()
                status = item.get("status", "unclear")
                reason = item.get("reason", "").strip()
                
                # ì™„ì „íˆ ë¹„ì–´ìˆëŠ” í•­ëª©ì€ ì œì™¸
                if not name and not reason:
                    logger.debug(f"[ì›Œí¬í”Œë¡œìš°] ë¹„ì–´ìˆëŠ” criteria í•­ëª© ì œì™¸: {item}")
                    continue
                
                # nameì´ë‚˜ reasonì´ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
                validated_criteria.append({
                    "name": name or "ë²•ì  ê·¼ê±° í™•ì¸ í•„ìš”",
                    "status": status or "unclear",
                    "reason": reason or "ê´€ë ¨ ë²•ë ¹ ì •ë³´ë¥¼ í™•ì¸í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.",
                })
            
            # validated_criteriaê°€ ë¹„ì–´ìˆìœ¼ë©´ legal_basis ê¸°ë°˜ fallback
            if not validated_criteria and legal_basis and len(legal_basis) > 0:
                logger.warning("[ì›Œí¬í”Œë¡œìš°] ëª¨ë“  criteriaê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. legal_basis ê¸°ë°˜ fallback ì‹œë„")
                for basis in legal_basis[:3]:
                    validated_criteria.append({
                        "name": basis.get("title", "ë²•ì  ê·¼ê±°"),
                        "status": "unclear",
                        "reason": basis.get("snippet", "")[:300] if basis.get("snippet") else "ê´€ë ¨ ë²•ë ¹ ì •ë³´ë¥¼ í™•ì¸í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.",
                    })
            
            criteria = validated_criteria if validated_criteria else [{
                "name": "ë²•ì  ê·¼ê±° í™•ì¸ í•„ìš”",
                "status": "unclear",
                "reason": "ê´€ë ¨ ë²•ë ¹ ì •ë³´ë¥¼ í™•ì¸í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤."
            }]
        
        # 2. action_plan ê²€ì¦ ë° ì •ê·œí™”
        action_plan = result.get("action_plan", {})
        if not isinstance(action_plan, dict):
            action_plan = {"steps": []}
        
        steps = action_plan.get("steps", [])
        if not isinstance(steps, list):
            steps = []
        
        # ê° step ê²€ì¦
        validated_steps = []
        for step in steps:
            if not isinstance(step, dict):
                continue
            
            title = step.get("title", "")
            items = step.get("items", [])
            
            # itemsê°€ ë°°ì—´ì´ ì•„ë‹ˆë©´ ë³€í™˜ ì‹œë„
            if not isinstance(items, list):
                if isinstance(items, str):
                    # ë¬¸ìì—´ì„ ë°°ì—´ë¡œ ë³€í™˜ (ì¤„ë°”ê¿ˆ ê¸°ì¤€)
                    items = [item.strip() for item in items.split('\n') if item.strip()]
                elif isinstance(items, dict):
                    # ê°ì²´ë¥¼ ë°°ì—´ë¡œ ë³€í™˜ (ê°’ë§Œ ì¶”ì¶œ)
                    items = [str(v) for v in items.values() if v]
                else:
                    items = []
            
            # itemsì—ì„œ ë§ˆí¬ë‹¤ìš´ ì¡°ê° ì œê±° (ì˜ˆ: "- " ì œê±°) ë° ì¤‘ë³µ í•„í„°ë§
            # ì‹ ê³ /ìƒë‹´ ê´€ë ¨ í•­ëª©ì„ ì „ë¶€ ì‚­ì œí•˜ì§€ ì•Šê³ , í•˜ë“œ ì œì™¸ë§Œ ì ìš©í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ë¶„ë¦¬
            # í•˜ë“œ ì œì™¸: ì „í™”ë²ˆí˜¸ + ìƒë‹´ì„¼í„° ê°™ì´ ë…¸ê³¨ì ì¸ ê²ƒë§Œ
            hard_exclude_keywords = [
                r'\d+.*ìƒë‹´ì„¼í„°',  # ì „í™”ë²ˆí˜¸ + ìƒë‹´ì„¼í„°
                r'ì²­ë…„ë…¸ë™ì„¼í„°',
            ]
            
            # ìƒë‹´/ì‹ ê³  ê´€ë ¨ í‚¤ì›Œë“œ (ë³„ë„ ë¶„ë¥˜ìš©)
            consult_keywords = [
                r'ë…¸ë¬´ì‚¬',
                r'ë…¸ë™ì²­',
                r'ê³ ìš©ë…¸ë™ë¶€',
                r'ìƒë‹´',
                r'ì‹ ê³ ',
            ]
            
            normal_items = []
            consult_items = []
            
            for item in items:
                if isinstance(item, str):
                    # "- " ë˜ëŠ” "* " ì œê±°
                    cleaned = re.sub(r'^[-*]\s+', '', item.strip())
                    if not cleaned:
                        continue
                    
                    # í•˜ë“œ ì œì™¸: ë„ˆë¬´ ë…¸ê³¨ì ì¸ "ê¸°ê´€ í™ë³´/ì „í™”ë²ˆí˜¸" ë¥˜ë§Œ ì™„ì „ ì œì™¸
                    should_hard_exclude = any(
                        re.search(pattern, cleaned, re.IGNORECASE)
                        for pattern in hard_exclude_keywords
                    )
                    if should_hard_exclude:
                        logger.debug(f"[ì›Œí¬í”Œë¡œìš°] í•˜ë“œ ì œì™¸: {cleaned}")
                        continue
                    
                    # ìƒë‹´/ì‹ ê³  ê´€ë ¨ì´ë©´ ë”°ë¡œ ëª¨ì•„ë‘ê¸°
                    is_consult = any(
                        re.search(pattern, cleaned, re.IGNORECASE)
                        for pattern in consult_keywords
                    )
                    
                    if is_consult:
                        consult_items.append(cleaned)
                    else:
                        normal_items.append(cleaned)
            
            # ìš°ì„  normal_itemsì—ì„œ ìµœëŒ€ 3ê°œ
            cleaned_items = normal_items[:3]
            
            # normalì´ ë„ˆë¬´ ì ìœ¼ë©´ ìƒë‹´ ê³„ì—´ì—ì„œ 1~2ê°œê¹Œì§€ ë³´ì¶©
            if len(cleaned_items) < 2 and consult_items:
                additional_count = min(2 - len(cleaned_items), len(consult_items))
                cleaned_items.extend(consult_items[:additional_count])
                logger.debug(f"[ì›Œí¬í”Œë¡œìš°] ìƒë‹´ ê´€ë ¨ í•­ëª© {additional_count}ê°œ ë³´ì¶© (step: {title})")
            
            # ìµœëŒ€ 3ê°œ í•­ëª©ìœ¼ë¡œ ì œí•œ (ê° stepë³„)
            if len(cleaned_items) > 3:
                logger.debug(f"[ì›Œí¬í”Œë¡œìš°] í•­ëª© ìˆ˜ ì œí•œ: {len(cleaned_items)}ê°œ â†’ 3ê°œ (step: {title})")
                cleaned_items = cleaned_items[:3]
            
            if title or cleaned_items:  # titleì´ ì—†ì–´ë„ itemsê°€ ìˆìœ¼ë©´ ìœ ì§€
                # cleaned_itemsê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ ì¶”ê°€
                if cleaned_items:
                    validated_steps.append({
                        "title": title or "ê¸°íƒ€",
                        "items": cleaned_items,
                    })
        
        # stepsê°€ ë¹„ì–´ìˆê±°ë‚˜ ëª¨ë“  stepì˜ itemsê°€ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ê°’
        has_any_items = any(step.get("items", []) for step in validated_steps)
        if not validated_steps or not has_any_items:
            # classified_typeì— ë”°ë¥¸ ê¸°ë³¸ action items
            classified_type = result.get("classified_type", "unknown")
            default_items_by_type = {
                "unpaid_wage": [
                    "ê·¼ë¡œê³„ì•½ì„œì™€ ê¸‰ì—¬ëª…ì„¸ì„œë¥¼ í™•ì¸í•˜ì„¸ìš”",
                    "ì¶œí‡´ê·¼ ê¸°ë¡ê³¼ ê·¼ë¬´ì‹œê°„ì„ ì •ë¦¬í•˜ì„¸ìš”",
                    "ì„ê¸ˆ ì§€ê¸‰ ë‚´ì—­ì„ ë¬¸ì„œë¡œ ë³´ê´€í•˜ì„¸ìš”"
                ],
                "harassment": [
                    "ê´´ë¡­í˜ ê´€ë ¨ ì¦ê±° ìë£Œë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”",
                    "ëŒ€í™” ë‚´ìš©ê³¼ ì¼ì‹œë¥¼ ê¸°ë¡í•˜ì„¸ìš”",
                    "ìƒí™©ì„ ê°ê´€ì ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”"
                ],
                "unfair_dismissal": [
                    "í•´ê³  í†µì§€ì„œì™€ ê´€ë ¨ ë¬¸ì„œë¥¼ ë³´ê´€í•˜ì„¸ìš”",
                    "ê·¼ë¬´ ê¸°ê°„ê³¼ ì„±ê³¼ë¥¼ ì •ë¦¬í•˜ì„¸ìš”",
                    "íšŒì‚¬ì™€ì˜ ëŒ€í™” ë‚´ìš©ì„ ê¸°ë¡í•˜ì„¸ìš”"
                ],
                "overtime": [
                    "ê·¼ë¬´ì‹œê°„ ê¸°ë¡ì„ í™•ì¸í•˜ì„¸ìš”",
                    "ì—°ì¥ê·¼ë¡œ ì‹œê°„ì„ ê³„ì‚°í•˜ì„¸ìš”",
                    "íœ´ê²Œì‹œê°„ ì¤€ìˆ˜ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”"
                ],
                "probation": [
                    "ìˆ˜ìŠµ ê¸°ê°„ê³¼ ì¡°ê±´ì„ í™•ì¸í•˜ì„¸ìš”",
                    "ê·¼ë¡œê³„ì•½ì„œì˜ ìˆ˜ìŠµ ì¡°í•­ì„ ê²€í† í•˜ì„¸ìš”",
                    "ìˆ˜ìŠµ ê¸°ê°„ ì¤‘ í‰ê°€ ë‚´ìš©ì„ ì •ë¦¬í•˜ì„¸ìš”"
                ],
            }
            default_items = default_items_by_type.get(classified_type, [
                "ìƒí™©ì„ ê°ê´€ì ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”",
                "ê´€ë ¨ ë¬¸ì„œë¥¼ ë³´ê´€í•˜ì„¸ìš”",
                "ì¦ê±° ìë£Œë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”"
            ])
            
            validated_steps = [{
                "title": "ì¦‰ì‹œ ì¡°ì¹˜",
                "items": default_items[:3]
            }]
            logger.info(f"[ì›Œí¬í”Œë¡œìš°] ê¸°ë³¸ action_plan ìƒì„± (classified_type: {classified_type})")
        
        action_plan = {"steps": validated_steps}
        
        # 3. scripts ê²€ì¦
        scripts = result.get("scripts", {})
        if not isinstance(scripts, dict):
            scripts = {}
        
        validated_scripts = {
            "to_company": scripts.get("to_company", "") if isinstance(scripts.get("to_company"), str) else "",
            "to_advisor": scripts.get("to_advisor", "") if isinstance(scripts.get("to_advisor"), str) else "",
        }
        
        result["criteria"] = criteria
        result["action_plan"] = action_plan
        result["scripts"] = validated_scripts
        
        # 3. organizations ê²€ì¦ ë° ì •ê·œí™”
        organizations = result.get("organizations", [])
        if not organizations or len(organizations) == 0:
            logger.warning("[ì›Œí¬í”Œë¡œìš°] organizationsê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ organizations ìƒì„±")
            # classified_typeì— ë”°ë¼ ê¸°ë³¸ ê¸°ê´€ ìƒì„±
            classified_type = result.get("classified_type", "unknown")
            default_orgs = {
                "unpaid_wage": ["moel", "labor_attorney", "comwel"],
                "harassment": ["moel_complaint", "human_rights", "labor_attorney"],
                "unfair_dismissal": ["moel", "labor_attorney", "comwel"],
                "overtime": ["moel", "labor_attorney", "comwel"],
                "probation": ["moel", "labor_attorney", "comwel"],
                "unknown": ["labor_attorney", "moel", "comwel"],
            }
            org_ids = default_orgs.get(classified_type, default_orgs["unknown"])
            # ê¸°ë³¸ ê¸°ê´€ ì •ë³´
            org_map = {
                "moel": {
                    "id": "moel",
                    "name": "ë…¸ë™ì²­",
                    "description": "ì²´ë¶ˆì„ê¸ˆ ì¡°ì‚¬ ë° ì‹œì • ëª…ë ¹, ê·¼ë¡œê¸°ì¤€ë²• ìœ„ë°˜ ì¡°ì‚¬",
                    "capabilities": ["ì²´ë¶ˆì„ê¸ˆ ì¡°ì‚¬", "ì‹œì • ëª…ë ¹", "ê·¼ë¡œê¸°ì¤€ë²• ìœ„ë°˜ ì¡°ì‚¬"],
                    "requiredDocs": ["ê·¼ë¡œê³„ì•½ì„œ", "ì¶œí‡´ê·¼ ê¸°ë¡", "ê¸‰ì—¬ëª…ì„¸ì„œ"],
                    "legalBasis": "ê·¼ë¡œê¸°ì¤€ë²• ì œ110ì¡°: ê·¼ë¡œê°ë…ê´€ì˜ ê¶Œí•œ",
                    "website": "https://www.moel.go.kr",
                    "phone": "1350"
                },
                "labor_attorney": {
                    "id": "labor_attorney",
                    "name": "ë…¸ë¬´ì‚¬",
                    "description": "ìƒë‹´ ë° ì†Œì†¡ ëŒ€ë¦¬, ê·¼ë¡œ ë¶„ìŸ í•´ê²° ì „ë¬¸",
                    "capabilities": ["ìƒë‹´", "ì†Œì†¡ ëŒ€ë¦¬", "ê·¼ë¡œ ë¶„ìŸ í•´ê²°"],
                    "requiredDocs": ["ê·¼ë¡œê³„ì•½ì„œ", "ë¬¸ì/ì¹´í†¡ ëŒ€í™”", "ê¸°íƒ€ ì¦ê±° ìë£Œ"],
                    "legalBasis": "ë…¸ë¬´ì‚¬ë²•: ê·¼ë¡œ ë¶„ìŸ ì „ë¬¸ ë²•ë¥  ì„œë¹„ìŠ¤"
                },
                "comwel": {
                    "id": "comwel",
                    "name": "ê·¼ë¡œë³µì§€ê³µë‹¨",
                    "description": "ì—°ì°¨ìˆ˜ë‹¹, íœ´ì¼ìˆ˜ë‹¹, ì‹¤ì—…ê¸‰ì—¬ ìƒë‹´",
                    "capabilities": ["ì—°ì°¨ìˆ˜ë‹¹ ìƒë‹´", "íœ´ì¼ìˆ˜ë‹¹ ìƒë‹´", "ì‹¤ì—…ê¸‰ì—¬ ì•ˆë‚´"],
                    "requiredDocs": ["ê·¼ë¡œê³„ì•½ì„œ", "ì¶œí‡´ê·¼ ê¸°ë¡", "ê¸‰ì—¬ëª…ì„¸ì„œ"],
                    "legalBasis": "ê·¼ë¡œê¸°ì¤€ë²• ì œ60ì¡°: ì—°ì°¨ ìœ ê¸‰íœ´ê°€",
                    "website": "https://www.comwel.or.kr",
                    "phone": "1588-0075"
                },
                "moel_complaint": {
                    "id": "moel_complaint",
                    "name": "ê³ ìš©ë…¸ë™ë¶€ ê³ ê°ìƒë‹´ì„¼í„°",
                    "description": "ì§ì¥ ë‚´ ê´´ë¡­í˜, ì°¨ë³„ ìƒë‹´ ë° ì¡°ì‚¬, ê³ ìš©Â·ë…¸ë™ ì „ë°˜ ìƒë‹´",
                    "capabilities": ["ì§ì¥ ë‚´ ê´´ë¡­í˜ ìƒë‹´", "ì°¨ë³„ ìƒë‹´", "ì¡°ì‚¬ ì§€ì›", "ê³ ìš©Â·ë…¸ë™ ì „ë°˜ ìƒë‹´"],
                    "requiredDocs": ["ì¦ê±° ìë£Œ", "ë¬¸ì/ì¹´í†¡ ëŒ€í™”", "ë…¹ìŒ íŒŒì¼"],
                    "legalBasis": "ì§ì¥ ë‚´ ê´´ë¡­í˜ ë°©ì§€ë²• ì œ13ì¡°: ê³ ì¶© ì²˜ë¦¬",
                    "website": "https://1350.moel.go.kr/home/hp/main/hpmain.do",
                    "phone": "1350"
                },
                "human_rights": {
                    "id": "human_rights",
                    "name": "êµ­ê°€ì¸ê¶Œìœ„ì›íšŒ",
                    "description": "ì¸ê¶Œ ì¹¨í•´ ìƒë‹´ ë° ì¡°ì‚¬, ì°¨ë³„ êµ¬ì œ",
                    "capabilities": ["ì¸ê¶Œ ì¹¨í•´ ìƒë‹´", "ì°¨ë³„ êµ¬ì œ", "ì¡°ì‚¬ ë° êµ¬ì œ"],
                    "requiredDocs": ["ì¦ê±° ìë£Œ", "ì°¨ë³„ ì‚¬ë¡€ ê¸°ë¡"],
                    "legalBasis": "êµ­ê°€ì¸ê¶Œìœ„ì›íšŒë²•: ì¸ê¶Œ ì¹¨í•´ êµ¬ì œ",
                    "website": "https://www.humanrights.go.kr",
                    "phone": "1331"
                }
            }
            organizations = [org_map.get(org_id, {}) for org_id in org_ids if org_id in org_map]
        else:
            # organizations êµ¬ì¡° ê²€ì¦
            validated_orgs = []
            for org in organizations:
                if isinstance(org, dict):
                    validated_orgs.append({
                        "id": org.get("id", ""),
                        "name": org.get("name", ""),
                        "description": org.get("description", ""),
                        "capabilities": org.get("capabilities", []),
                        "requiredDocs": org.get("requiredDocs", []),
                        "legalBasis": org.get("legalBasis"),
                        "website": org.get("website"),
                        "phone": org.get("phone"),
                    })
            organizations = validated_orgs
        
        result["organizations"] = organizations
        
        # 4. summary ê²€ì¦ (4ê°œ ì„¹ì…˜ í™•ì¸)
        summary = result.get("summary", "")
        if not isinstance(summary, str):
            summary = ""
        
        # summaryì— 4ê°œ ì„¹ì…˜ì´ ëª¨ë‘ ìˆëŠ”ì§€ í™•ì¸ (ìœ ì—°í•œ ë§¤ì¹­)
        section_patterns = [
            {
                "title": "## ğŸ“Š ìƒí™© ë¶„ì„ì˜ ê²°ê³¼",
                "keywords": ["ìƒí™© ë¶„ì„ì˜ ê²°ê³¼", "ìƒí™© ë¶„ì„", "ë¶„ì„ì˜ ê²°ê³¼"],
                "emoji": "ğŸ“Š"
            },
            {
                "title": "## âš–ï¸ ë²•ì  ê´€ì ì—ì„œ ë³¸ í˜„ì¬ìƒí™©",
                "keywords": ["ë²•ì  ê´€ì ", "ë²•ì  ê´€ì ì—ì„œ ë³¸ í˜„ì¬ìƒí™©", "ë²•ì  ê´€ì ì—ì„œ"],
                "emoji": "âš–ï¸"
            },
            {
                "title": "## ğŸ¯ ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™",
                "keywords": ["ì§€ê¸ˆ ë‹¹ì¥ í•  ìˆ˜ ìˆëŠ” í–‰ë™", "í•  ìˆ˜ ìˆëŠ” í–‰ë™", "í–‰ë™"],
                "emoji": "ğŸ¯"
            },
            {
                "title": "## ğŸ’¬ ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”",
                "keywords": ["ì´ë ‡ê²Œ ë§í•´ë³´ì„¸ìš”", "ë§í•´ë³´ì„¸ìš”"],
                "emoji": "ğŸ’¬"
            },
        ]
        
        # ì„¹ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ìœ ì—°í•œ ë§¤ì¹­)
        found_sections = []
        missing_sections = []
        
        for section_info in section_patterns:
            found = False
            # ì •í™•í•œ í—¤ë” í˜•ì‹ í™•ì¸
            if section_info["title"] in summary:
                found = True
            else:
                # í‚¤ì›Œë“œë¡œ í™•ì¸ (ì´ëª¨ì§€ì™€ ## ì—†ì´ë„ ë§¤ì¹­)
                for keyword in section_info["keywords"]:
                    # ë§ˆí¬ë‹¤ìš´ í—¤ë” í˜•ì‹ (## í‚¤ì›Œë“œ ë˜ëŠ” # í‚¤ì›Œë“œ)
                    pattern1 = rf'##?\s*{re.escape(keyword)}'
                    pattern2 = rf'##?\s*.*?{re.escape(keyword)}'
                    if re.search(pattern1, summary, re.IGNORECASE) or re.search(pattern2, summary, re.IGNORECASE):
                        found = True
                        break
                    # ì´ëª¨ì§€ì™€ í•¨ê»˜ ìˆëŠ” ê²½ìš°
                    pattern3 = rf'##?\s*.*?{re.escape(keyword)}'
                    if re.search(pattern3, summary, re.IGNORECASE):
                        found = True
                        break
            
            if found:
                found_sections.append(section_info["title"])
            else:
                missing_sections.append(section_info)
        
        if missing_sections:
            logger.warning(f"[ì›Œí¬í”Œë¡œìš°] summaryì— ëˆ„ë½ëœ ì„¹ì…˜: {[s['title'] for s in missing_sections]}")
            
            # LLMì´ ìƒì„±í•œ ë‚´ìš©ì„ íŒŒì‹±í•˜ì—¬ ì„¹ì…˜ ì¬êµ¬ì„± ì‹œë„
            classified_type = result.get("classified_type", "unknown")
            summary_reconstructed = self._reconstruct_summary_sections(summary, section_patterns, classified_type)
            if summary_reconstructed:
                summary = summary_reconstructed
                logger.info("[ì›Œí¬í”Œë¡œìš°] summary ì„¹ì…˜ ì¬êµ¬ì„± ì™„ë£Œ")
            else:
                # ì¬êµ¬ì„± ì‹¤íŒ¨ ì‹œ ëˆ„ë½ëœ ì„¹ì…˜ ì¶”ê°€ (ì¹´í…Œê³ ë¦¬ë³„ ê¸°ë³¸ê°’ ì œê³µ)
                classified_type = result.get("classified_type", "unknown")
                default_content_by_type = {
                    "unpaid_wage": {
                        "âš–ï¸": "ì„ê¸ˆì²´ë¶ˆì€ ê·¼ë¡œê¸°ì¤€ë²• ì œ43ì¡°(ì„ê¸ˆì§€ê¸‰), ì œ36ì¡°(ì„ê¸ˆì˜ ì§€ê¸‰)ì™€ ê´€ë ¨ëœ ì‚¬ì•ˆì…ë‹ˆë‹¤. ì‚¬ìš©ìëŠ” ê·¼ë¡œìì—ê²Œ ì„ê¸ˆì„ ì •ê¸°ì ìœ¼ë¡œ ì§€ê¸‰í•  ì˜ë¬´ê°€ ìˆìœ¼ë©°, ì´ë¥¼ ìœ„ë°˜í•  ê²½ìš° í˜•ì‚¬ì²˜ë²Œê³¼ ë¯¼ì‚¬ìƒ ì†í•´ë°°ìƒ ì±…ì„ì„ ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                        "ğŸ¯": "- ê·¼ë¡œê³„ì•½ì„œì™€ ê¸‰ì—¬ëª…ì„¸ì„œë¥¼ í™•ì¸í•˜ì„¸ìš”\n- ì¶œí‡´ê·¼ ê¸°ë¡ê³¼ ê·¼ë¬´ì‹œê°„ì„ ì •ë¦¬í•˜ì„¸ìš”\n- ì„ê¸ˆ ì§€ê¸‰ ë‚´ì—­ì„ ë¬¸ì„œë¡œ ë³´ê´€í•˜ì„¸ìš”",
                        "ğŸ’¬": "íšŒì‚¬ì— ì •ì¤‘í•˜ê²Œ ì„ê¸ˆ ì§€ê¸‰ì„ ìš”ì²­í•˜ëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                    },
                    "harassment": {
                        "âš–ï¸": "ì§ì¥ ë‚´ ê´´ë¡­í˜ì€ ì§ì¥ ë‚´ ê´´ë¡­í˜ ë°©ì§€ ë° ê·¼ë¡œì ë³´í˜¸ ë“±ì— ê´€í•œ ë²•ë¥ ì— ë”°ë¼ ê¸ˆì§€ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì—…ë¬´ìƒ ì§€ìœ„ë‚˜ ê´€ê³„ë¥¼ ì´ìš©í•˜ì—¬ ê·¼ë¡œìì—ê²Œ ì‹ ì²´ì Â·ì •ì‹ ì  ê³ í†µì„ ì£¼ëŠ” í–‰ìœ„ëŠ” ë²•ì  ì²˜ë²Œ ëŒ€ìƒì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                        "ğŸ¯": "- ê´´ë¡­í˜ ê´€ë ¨ ì¦ê±° ìë£Œë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”\n- ëŒ€í™” ë‚´ìš©ê³¼ ì¼ì‹œë¥¼ ê¸°ë¡í•˜ì„¸ìš”\n- ìƒí™©ì„ ê°ê´€ì ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”",
                        "ğŸ’¬": "íšŒì‚¬ì— ê´´ë¡­í˜ ìƒí™©ì„ ì •ì¤‘í•˜ê²Œ ì•Œë¦¬ëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                    },
                    "unfair_dismissal": {
                        "âš–ï¸": "ë¶€ë‹¹í•´ê³ ëŠ” ê·¼ë¡œê¸°ì¤€ë²• ì œ23ì¡°(í•´ê³ ì˜ ì œí•œ)ì— ë”°ë¼ ì œí•œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì •ë‹¹í•œ ì‚¬ìœ  ì—†ì´ í•´ê³ í•˜ëŠ” ê²½ìš° ë³µì§ ì²­êµ¬ë‚˜ ì†í•´ë°°ìƒ ì²­êµ¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
                        "ğŸ¯": "- í•´ê³  í†µì§€ì„œì™€ ê´€ë ¨ ë¬¸ì„œë¥¼ ë³´ê´€í•˜ì„¸ìš”\n- ê·¼ë¬´ ê¸°ê°„ê³¼ ì„±ê³¼ë¥¼ ì •ë¦¬í•˜ì„¸ìš”\n- íšŒì‚¬ì™€ì˜ ëŒ€í™” ë‚´ìš©ì„ ê¸°ë¡í•˜ì„¸ìš”",
                        "ğŸ’¬": "íšŒì‚¬ì— í•´ê³  ì‚¬ìœ ì— ëŒ€í•œ ì„¤ëª…ì„ ìš”ì²­í•˜ëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                    },
                    "overtime": {
                        "âš–ï¸": "ê·¼ë¡œì‹œê°„ì€ ê·¼ë¡œê¸°ì¤€ë²• ì œ50ì¡°(ê·¼ë¡œì‹œê°„), ì œ53ì¡°(ì—°ì¥ê·¼ë¡œ)ì— ë”°ë¼ ê·œì œë©ë‹ˆë‹¤. ë²•ì • ê·¼ë¡œì‹œê°„ì„ ì´ˆê³¼í•˜ëŠ” ì—°ì¥ê·¼ë¡œì— ëŒ€í•´ì„œëŠ” ê°€ì‚°ì„ê¸ˆì„ ì§€ê¸‰í•´ì•¼ í•©ë‹ˆë‹¤.",
                        "ğŸ¯": "- ê·¼ë¬´ì‹œê°„ ê¸°ë¡ì„ í™•ì¸í•˜ì„¸ìš”\n- ì—°ì¥ê·¼ë¡œ ì‹œê°„ì„ ê³„ì‚°í•˜ì„¸ìš”\n- íœ´ê²Œì‹œê°„ ì¤€ìˆ˜ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì„¸ìš”",
                        "ğŸ’¬": "íšŒì‚¬ì— ê·¼ë¡œì‹œê°„ê³¼ ê°€ì‚°ì„ê¸ˆì— ëŒ€í•´ ë¬¸ì˜í•˜ëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                    },
                    "probation": {
                        "âš–ï¸": "ìˆ˜ìŠµê¸°ê°„ì€ ê·¼ë¡œê¸°ì¤€ë²•ì— ë”°ë¼ í•©ë¦¬ì ì¸ ë²”ìœ„ ë‚´ì—ì„œë§Œ ì¸ì •ë©ë‹ˆë‹¤. ìˆ˜ìŠµê¸°ê°„ ì¤‘ì—ë„ ê·¼ë¡œê¸°ì¤€ë²•ìƒ ë³´í˜¸ë¥¼ ë°›ìœ¼ë©°, ë¶€ë‹¹í•œ í•´ê³ ëŠ” ì œí•œë©ë‹ˆë‹¤.",
                        "ğŸ¯": "- ìˆ˜ìŠµ ê¸°ê°„ê³¼ ì¡°ê±´ì„ í™•ì¸í•˜ì„¸ìš”\n- ê·¼ë¡œê³„ì•½ì„œì˜ ìˆ˜ìŠµ ì¡°í•­ì„ ê²€í† í•˜ì„¸ìš”\n- ìˆ˜ìŠµ ê¸°ê°„ ì¤‘ í‰ê°€ ë‚´ìš©ì„ ì •ë¦¬í•˜ì„¸ìš”",
                        "ğŸ’¬": "íšŒì‚¬ì— ìˆ˜ìŠµê¸°ê°„ê³¼ í‰ê°€ ê¸°ì¤€ì— ëŒ€í•´ ë¬¸ì˜í•˜ëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                    },
                }
                
                default_content = default_content_by_type.get(classified_type, {
                    "âš–ï¸": "ê´€ë ¨ ë²•ë ¹ì„ í™•ì¸í•˜ì—¬ í˜„ì¬ ìƒí™©ì„ ë²•ì ìœ¼ë¡œ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤.",
                    "ğŸ¯": "- ìƒí™©ì„ ê°ê´€ì ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”\n- ê´€ë ¨ ë¬¸ì„œë¥¼ ë³´ê´€í•˜ì„¸ìš”\n- ì¦ê±° ìë£Œë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”",
                    "ğŸ’¬": "íšŒì‚¬ë‚˜ ìƒë‹´ ê¸°ê´€ì— ìƒí™©ì„ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ë¬¸êµ¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”."
                })
                
                # ëˆ„ë½ëœ ì„¹ì…˜ ì¶”ê°€
                for section_info in missing_sections:
                    emoji = section_info.get("emoji", "")
                    default_text = default_content.get(emoji, "í•´ë‹¹ ì„¹ì…˜ ë‚´ìš©ì„ í™•ì¸í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤.")
                    summary += f"\n\n{section_info['title']}\n\n{default_text}"
        
        return {
            "summary": summary,
            "criteria": criteria,
            "action_plan": action_plan,
            "scripts": validated_scripts,
            "organizations": organizations,
        }
    
    async def _call_llm(self, prompt: str) -> str:
        """LLM í˜¸ì¶œ (Groq/Ollama)"""
        from config import settings
        
        if settings.use_groq:
            from llm_api import ask_groq_with_messages
            messages = [
                {"role": "system", "content": "ë„ˆëŠ” ìœ ëŠ¥í•œ ë²•ë¥  AIì•¼. í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ]
            return ask_groq_with_messages(
                messages=messages,
                temperature=settings.llm_temperature,
                model=settings.groq_model
            )
        elif settings.use_ollama:
            # Ollama ì‚¬ìš©
            try:
                from langchain_ollama import OllamaLLM
                llm = OllamaLLM(
                    base_url=settings.ollama_base_url,
                    model=settings.ollama_model
                )
            except ImportError:
                # ëŒ€ì•ˆ: langchain-community ì‚¬ìš©
                from langchain_community.llms import Ollama
                llm = Ollama(
                    base_url=settings.ollama_base_url,
                    model=settings.ollama_model
                )
            
            response_text = llm.invoke(prompt)
            return response_text
        else:
            # Groqì™€ Ollama ëª¨ë‘ ì‚¬ìš© ì•ˆ í•¨
            raise ValueError("LLMì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. LLM_PROVIDER í™˜ê²½ë³€ìˆ˜ë¥¼ 'groq' ë˜ëŠ” 'ollama'ë¡œ ì„¤ì •í•˜ì„¸ìš”.")
    
    # ==================== ê³µê°œ ë©”ì„œë“œ ====================
    
    async def run(self, initial_state: Dict[str, Any]) -> Dict[str, Any]:
        """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        logger.info("[ì›Œí¬í”Œë¡œìš°] ì‹¤í–‰ ì‹œì‘")
        
        # Stateë¡œ ë³€í™˜
        state: SituationWorkflowState = {
            "situation_text": initial_state.get("situation_text", ""),
            "category_hint": initial_state.get("category_hint"),
            "summary": initial_state.get("summary"),
            "details": initial_state.get("details"),
            "employment_type": initial_state.get("employment_type"),
            "work_period": initial_state.get("work_period"),
            "weekly_hours": initial_state.get("weekly_hours"),
            "is_probation": initial_state.get("is_probation"),
            "social_insurance": initial_state.get("social_insurance"),
            "query_text": None,
            "query_embedding": None,
            "classification": None,
            "filtered_categories": None,
            "grounding_chunks": None,
            "related_cases": None,
            "legal_basis": None,
            "action_plan": None,
            "scripts": None,
            "criteria": None,
            "summary_report": None,
            "final_output": None,
        }
        
        # ê·¸ë˜í”„ ì‹¤í–‰
        final_state = await self.graph.ainvoke(state)
        
        # ìµœì¢… ì¶œë ¥ ë°˜í™˜
        return final_state.get("final_output", {})

