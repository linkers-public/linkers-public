"""
backend/data/legal/ í´ë”ì˜ ëª¨ë“  íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

ëª¨ë“  legal ë°ì´í„°ëŠ” legal_chunks í…Œì´ë¸”ì— ì €ì¥í•©ë‹ˆë‹¤.
- standard_contracts/ â†’ legal_chunks (source_type: "standard_contract")
- laws/ â†’ legal_chunks (source_type: "law")
- manuals/ â†’ legal_chunks (source_type: "manual")
- cases/ â†’ legal_chunks (source_type: "case")
"""

import os
import sys
import asyncio
from pathlib import Path
from typing import List, Dict, Any, Optional
import uuid
import hashlib

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
backend_dir = Path(__file__).parent.parent
sys.path.insert(0, str(backend_dir))

from core.document_processor_v2 import DocumentProcessor
from core.generator_v2 import LLMGenerator
from core.supabase_vector_store import SupabaseVectorStore
from core.logging_config import get_logger

logger = get_logger(__name__)


def get_source_type_from_path(file_path: Path) -> str:
    """íŒŒì¼ ê²½ë¡œì—ì„œ source_type ì¶”ì¶œ"""
    path_str = str(file_path)
    if "standard_contracts" in path_str:
        return "standard_contract"
    elif "laws" in path_str:
        return "law"
    elif "manuals" in path_str:
        return "manual"
    elif "cases" in path_str:
        return "case"
    else:
        return "unknown"




async def process_legal_file(
    file_path: Path,
    processor: DocumentProcessor,
    generator: LLMGenerator,
    vector_store: SupabaseVectorStore,
) -> Dict[str, Any]:
    """
    ëª¨ë“  legal íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ legal_chunksì— ì €ì¥
    
    Returns:
        {
            "file": str,
            "status": "success" | "failed",
            "external_id": str,
            "chunks_count": int,
            "error": str (optional)
        }
    """
    file_name = file_path.name
    source_type = get_source_type_from_path(file_path)
    
    # external_id ìƒì„± (íŒŒì¼ ê²½ë¡œ ê¸°ë°˜ í•´ì‹œ)
    relative_path = str(file_path.relative_to(backend_dir))
    external_id = hashlib.md5(relative_path.encode()).hexdigest()
    
    try:
        # 0. ì¤‘ë³µ ì²´í¬: ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ì¸ì§€ í™•ì¸
        logger.info(f"  ğŸ“„ ì†ŒìŠ¤ íƒ€ì…: {source_type}")
        logger.info(f"  ğŸ” ì¤‘ë³µ ì²´í¬ ì¤‘... (external_id: {external_id[:8]}...)")
        
        if vector_store.check_legal_chunks_exist(external_id):
            logger.info(f"  â­ï¸  ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ì…ë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            # ê¸°ì¡´ ì²­í¬ ê°œìˆ˜ í™•ì¸
            try:
                result = vector_store.sb.table("legal_chunks")\
                    .select("id", count="exact")\
                    .eq("external_id", external_id)\
                    .execute()
                existing_count = result.count if result.count is not None else len(result.data) if result.data else 0
                logger.info(f"  â„¹ï¸  ê¸°ì¡´ ì²­í¬ ê°œìˆ˜: {existing_count}ê°œ")
            except:
                existing_count = 0
            
            return {
                "file": file_name,
                "status": "skipped",
                "external_id": external_id,
                "chunks_count": existing_count,
                "error": None
            }
        
        logger.info(f"  âœ“ ì‹ ê·œ íŒŒì¼ì…ë‹ˆë‹¤. ì²˜ë¦¬ ì‹œì‘...")
        
        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        logger.info(f"  ğŸ” í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
        extracted_text, _ = processor.process_file(str(file_path), file_type=None)
        
        if not extracted_text or extracted_text.strip() == "":
            return {
                "file": file_name,
                "status": "failed",
                "external_id": external_id,
                "chunks_count": 0,
                "error": "í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ (ë¹ˆ íŒŒì¼)"
            }
        
        logger.info(f"  âœ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(extracted_text):,}ì")
        
        # 2. ì²­í‚¹ (í‘œì¤€ê³„ì•½ì„œëŠ” ì¡°í•­ ë‹¨ìœ„, ë‚˜ë¨¸ì§€ëŠ” ì¼ë°˜ ì²­í‚¹)
        logger.info(f"  âœ‚ï¸  ì²­í‚¹ ì¤‘...")
        if source_type == "standard_contract":
            # í‘œì¤€ê³„ì•½ì„œëŠ” ì¡°í•­ ë‹¨ìœ„ ì²­í‚¹ ì‹œë„
            try:
                chunks = processor.to_contract_chunks(
                    text=extracted_text,
                    base_meta={
                        "external_id": external_id,
                        "source_type": source_type,
                        "title": file_name,
                        "filename": file_name,
                        "file_path": relative_path,
                    }
                )
                # ì¡°í•­ ë‹¨ìœ„ ì²­í‚¹ ì„±ê³µ ì‹œ ë©”íƒ€ë°ì´í„°ì— article_number ë“± í¬í•¨
            except:
                # ì¡°í•­ ë‹¨ìœ„ ì²­í‚¹ ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ì²­í‚¹ìœ¼ë¡œ í´ë°±
                chunks = processor.to_chunks(
                    text=extracted_text,
                    base_meta={
                        "external_id": external_id,
                        "source_type": source_type,
                        "title": file_name,
                        "filename": file_name,
                        "file_path": relative_path,
                    }
                )
        else:
            # ë²•ë ¹/ë§¤ë‰´ì–¼/ì¼€ì´ìŠ¤ëŠ” ì¼ë°˜ ì²­í‚¹
            chunks = processor.to_chunks(
                text=extracted_text,
                base_meta={
                    "external_id": external_id,
                    "source_type": source_type,
                    "title": file_name,
                    "filename": file_name,
                    "file_path": relative_path,
                }
            )
        
        if not chunks:
            return {
                "file": file_name,
                "status": "failed",
                "external_id": external_id,
                "chunks_count": 0,
                "error": "ì²­í¬ ìƒì„± ì‹¤íŒ¨"
            }
        
        logger.info(f"  âœ“ ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
        
        # 3. ì„ë² ë”© ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì†ë„ ê°œì„ )
        import time
        start_time = time.time()
        logger.info(f"  ğŸ§® ì„ë² ë”© ìƒì„± ì¤‘... ({len(chunks)}ê°œ ì²­í¬)")
        logger.info(f"     â±ï¸  ì˜ˆìƒ ì‹œê°„: ì•½ {len(chunks) * 0.3:.0f}~{len(chunks) * 1.0:.0f}ì´ˆ (CPU ëª¨ë“œ)")
        chunk_texts = [chunk.content for chunk in chunks]
        
        # ì„ë² ë”© ìƒì„± (ì§„í–‰ ìƒí™©ì€ sentence-transformersê°€ ìë™ìœ¼ë¡œ í‘œì‹œ)
        embeddings = generator.embed(chunk_texts)
        
        elapsed_time = time.time() - start_time
        logger.info(f"  âœ“ ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embeddings)}ê°œ")
        logger.info(f"     â±ï¸  ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ (í‰ê· : {elapsed_time/len(chunks):.3f}ì´ˆ/ì²­í¬)")
        
        # 4. legal_chunks í…Œì´ë¸”ì— ì €ì¥
        logger.info(f"  ğŸ’¾ DB ì €ì¥ ì¤‘...")
        # bulk_upsert_legal_chunksëŠ” metadata ì•ˆì— ì •ë³´ë¥¼ ë„£ì–´ì•¼ í•¨
        chunk_payload = []
        for idx, chunk in enumerate(chunks):
            # metadataì— ëª¨ë“  ì •ë³´ í¬í•¨ (bulk_upsert_legal_chunksê°€ metadataì—ì„œ ì¶”ì¶œ)
            chunk_metadata = {
                **chunk.metadata,
                "external_id": external_id,
                "source_type": source_type,
                "title": file_name,
                "filename": file_name,
                "file_path": relative_path,
                "chunk_index": chunk.index,
            }
            
            chunk_payload.append({
                "content": chunk.content,
                "embedding": embeddings[idx],
                "metadata": chunk_metadata,
            })
        
        vector_store.bulk_upsert_legal_chunks(chunk_payload)
        
        logger.info(f"  âœ“ ì €ì¥ ì™„ë£Œ: external_id={external_id[:8]}...")
        
        return {
            "file": file_name,
            "status": "success",
            "external_id": external_id,
            "chunks_count": len(chunk_payload),
            "error": None
        }
        
    except Exception as e:
        logger.error(f"[ì²˜ë¦¬ ì‹¤íŒ¨] {file_name}: {str(e)}", exc_info=True)
        return {
            "file": file_name,
            "status": "failed",
            "external_id": external_id if 'external_id' in locals() else None,
            "chunks_count": 0,
            "error": str(e)
        }


async def main():
    """ë©”ì¸ í•¨ìˆ˜: data/legal/ í´ë”ì˜ ëª¨ë“  íŒŒì¼ ì²˜ë¦¬"""
    
    # ë°ì´í„° í´ë” ê²½ë¡œ
    legal_dir = backend_dir / "data" / "legal"
    
    if not legal_dir.exists():
        logger.error(f"ë°ì´í„° í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {legal_dir}")
        return
    
    # ì§€ì› íŒŒì¼ í˜•ì‹
    supported_extensions = ['.pdf', '.hwp', '.hwpx', '.txt', '.md']
    
    # íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘ (ëª¨ë“  í•˜ìœ„ í´ë”)
    all_files = []
    
    # ëª¨ë“  í•˜ìœ„ í´ë”ì—ì„œ íŒŒì¼ ìˆ˜ì§‘
    for subfolder in ["standard_contracts", "laws", "manuals", "cases"]:
        subfolder_dir = legal_dir / subfolder
        if subfolder_dir.exists():
            for ext in supported_extensions:
                all_files.extend(list(subfolder_dir.glob(f"*{ext}")))
                all_files.extend(list(subfolder_dir.glob(f"**/*{ext}")))
    
    if not all_files:
        logger.warning(f"ì²˜ë¦¬í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {legal_dir}")
        return
    
    logger.info("=" * 60)
    logger.info(f"[ì‹œì‘] data/legal/ í´ë” ì „ì²´ ì²˜ë¦¬")
    logger.info(f"  - ì´ íŒŒì¼: {len(all_files)}ê°œ (ëª¨ë‘ legal_chunksì— ì €ì¥)")
    logger.info("=" * 60)
    
    # ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (í•œ ë²ˆë§Œ ì´ˆê¸°í™”í•˜ì—¬ ì†ë„ ê°œì„ )
    logger.info("[ì´ˆê¸°í™” ì¤‘] DocumentProcessor, LLMGenerator, SupabaseVectorStore...")
    processor = DocumentProcessor()
    generator = LLMGenerator()  # ì„ë² ë”© ëª¨ë¸ ë¡œë”© (ì²˜ìŒì—ë§Œ ëŠë¦¼)
    vector_store = SupabaseVectorStore()
    logger.info("[ì´ˆê¸°í™” ì™„ë£Œ]")
    
    # ê²°ê³¼ ì €ì¥
    results = []
    
    # ëª¨ë“  íŒŒì¼ ì²˜ë¦¬
    logger.info(f"\n[ì²˜ë¦¬ ì‹œì‘] ì´ {len(all_files)}ê°œ íŒŒì¼")
    logger.info("=" * 60)
    
    for idx, file_path in enumerate(all_files, 1):
        progress_percent = (idx / len(all_files)) * 100
        logger.info("")
        logger.info(f"[{idx}/{len(all_files)}] ({progress_percent:.1f}%) {file_path.name}")
        logger.info(f"  â””â”€ ê²½ë¡œ: {file_path.relative_to(backend_dir)}")
        
        result = await process_legal_file(
            file_path=file_path,
            processor=processor,
            generator=generator,
            vector_store=vector_store,
        )
        
        results.append({
            **result,
            "type": get_source_type_from_path(file_path),
            "target_table": "legal_chunks"
        })
        
        if result["status"] == "success":
            logger.info(f"  âœ… ì„±ê³µ: {result['chunks_count']}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ")
        elif result["status"] == "skipped":
            logger.info(f"  â­ï¸  ìŠ¤í‚µ: ì´ë¯¸ ì¡´ì¬í•¨ ({result['chunks_count']}ê°œ ì²­í¬)")
        else:
            logger.error(f"  âŒ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
        
        # ì§„í–‰ ìƒí™© ìš”ì•½ (10ê°œë§ˆë‹¤ ë˜ëŠ” ë§ˆì§€ë§‰ íŒŒì¼)
        if idx % 10 == 0 or idx == len(all_files):
            success_so_far = sum(1 for r in results if r["status"] == "success")
            skipped_so_far = sum(1 for r in results if r["status"] == "skipped")
            failed_so_far = sum(1 for r in results if r["status"] == "failed")
            logger.info(f"  ğŸ“Š í˜„ì¬ê¹Œì§€: ì„±ê³µ {success_so_far}ê°œ, ìŠ¤í‚µ {skipped_so_far}ê°œ, ì‹¤íŒ¨ {failed_so_far}ê°œ")
    
    logger.info("")
    logger.info("=" * 60)
    
    # ê²°ê³¼ ìš”ì•½ (source_typeë³„)
    success_count = sum(1 for r in results if r["status"] == "success")
    skipped_count = sum(1 for r in results if r["status"] == "skipped")
    failed_count = sum(1 for r in results if r["status"] == "failed")
    total_chunks = sum(r["chunks_count"] for r in results if r["status"] == "success")
    
    # source_typeë³„ í†µê³„
    type_stats = {}
    for r in results:
        source_type = r.get("type", "unknown")
        if source_type not in type_stats:
            type_stats[source_type] = {"total": 0, "success": 0, "skipped": 0, "failed": 0, "chunks": 0}
        type_stats[source_type]["total"] += 1
        if r["status"] == "success":
            type_stats[source_type]["success"] += 1
            type_stats[source_type]["chunks"] += r["chunks_count"]
        elif r["status"] == "skipped":
            type_stats[source_type]["skipped"] += 1
        else:
            type_stats[source_type]["failed"] += 1
    
    logger.info("=" * 60)
    logger.info(f"[ì™„ë£Œ] ì²˜ë¦¬ ê²°ê³¼:")
    logger.info(f"  - ì´ íŒŒì¼: {len(results)}ê°œ")
    for source_type, stats in type_stats.items():
        logger.info(f"    * {source_type}: {stats['total']}ê°œ (ì„±ê³µ: {stats['success']}ê°œ, ìŠ¤í‚µ: {stats['skipped']}ê°œ, ì‹¤íŒ¨: {stats['failed']}ê°œ, ì²­í¬: {stats['chunks']}ê°œ)")
    logger.info(f"  - ì„±ê³µ: {success_count}ê°œ")
    logger.info(f"  - ìŠ¤í‚µ: {skipped_count}ê°œ (ì´ë¯¸ ì¡´ì¬)")
    logger.info(f"  - ì‹¤íŒ¨: {failed_count}ê°œ")
    logger.info(f"  - ì‹ ê·œ ì €ì¥ ì²­í¬: {total_chunks}ê°œ")
    logger.info("=" * 60)
    
    # ì‹¤íŒ¨í•œ íŒŒì¼ ëª©ë¡
    if failed_count > 0:
        logger.warning("ì‹¤íŒ¨í•œ íŒŒì¼ ëª©ë¡:")
        for r in results:
            if r["status"] == "failed":
                logger.warning(f"  - {r['file']} ({r.get('target_table', 'unknown')}): {r.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
    
    # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    import json
    from datetime import datetime
    
    report_dir = backend_dir / "data" / "indexed" / "reports"
    report_dir.mkdir(parents=True, exist_ok=True)
    
    report_file = report_dir / f"legal_data_indexing_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    report = {
        "total": len(results),
        "by_source_type": type_stats,
        "summary": {
            "success": success_count,
            "skipped": skipped_count,
            "failed": failed_count,
            "total_chunks": total_chunks
        },
        "results": results,
        "processed_at": datetime.now().isoformat()
    }
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    logger.info(f"[ë¦¬í¬íŠ¸ ì €ì¥] {report_file}")


if __name__ == "__main__":
    asyncio.run(main())

